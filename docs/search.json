{
  "articles": [
    {
      "path": "ANDEVA.html",
      "title": "Script ANDEVA de una via",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nConceptos b√°sicos\r\ndel an√°lisis de la varianza\r\n\r\nC.1\r\nR.2\r\nR.3\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nConceptos b√°sicos\r\ndel an√°lisis de la varianza\r\nLa hip√≥tesis nula en un an√°lisis de la varianza tipo I com√∫n es:\r\n\\[H0: m1 = m2 = m3 = ... = mk\\]\r\n¬øC√≥mo es que esta hip√≥tesis se pone a prueba en un ANDEVA? Por cierto\r\nesta es una prueba ‚Äúomnibus‚Äù, es decir ¬°prueba muchas cosas de un\r\njal√≥n!\r\nPara ver como es que opera el anova veamos el ejemplo que sigue:\r\nTomemos un solo factor, ‚Äúf‚Äù, con dos niveles y pongamos los datos en una\r\ngr√°fica simple, seg√∫n el orden en el que fueron obtenidas las\r\nmediciones.\r\n\r\n\r\nanova<-read.table(\"anova.data.txt\",header=T)\r\nattach(anova)\r\nnames(anova)\r\n\r\n\r\n\r\n\r\n\r\nplot(y)\r\nabline(mean(y), 0, col=4)\r\nfor (i in 1:length(y)) lines (c(i,i), c(mean(y), y[i]), col=5)\r\n\r\n\r\n\r\n\r\n¬øQu√© muestra esta gr√°fica? ¬øA que equivale la suma de los trazos\r\nverticales?\r\n\\[ SCT=\\Sigma(y-\r\n\\overline{y})^2\\]\r\nAhora vamos a hacer exactamente lo mismo, pero ahora dividiendo por\r\ncada uno de los niveles del factor F. Incorporemos la informaci√≥n del\r\nfactor ‚Äúf‚Äù. Para esto hay que calcular los promedios de ‚Äúy‚Äù que\r\ncorresponden a los niveles de ‚Äúf‚Äù\r\n\r\n\r\npromedios <- tapply(y, f, mean)\r\n\r\n\r\n\r\nGrafiquemos esta nueva estructura de datos sobre la gr√°fica que ya\r\ntenemos\r\n\r\n\r\nplot(y)\r\nlines(c(1, 7), c(promedios[1], promedios[1]), col = 2)\r\nlines(c(7, 14), c(promedios[2], promedios[2]), col = 5)\r\nfor (i in 1:7 ) lines (c(i,i), c(promedios[1], y[i]), col = 1, lty=6)\r\nfor (i in 8:14) lines (c(i,i), c(promedios[2], y[i]), col = 1, lty=6)\r\n\r\n\r\n\r\n\r\n¬øQu√© muestra esta gr√°fica? ¬øa que equivale la suma de los trazos\r\npunteados verticales?\r\n\\[SCE= \\Sigma(y_1-\\hat{y_1})^2 +\r\n\\Sigma(y_2- \\hat{y_2})^2\\]\r\nSi las dos medias fueran iguales ¬øc√≥mo comparar√≠an estas dos\r\ngr√°ficas?\r\nSi lo piensan tendr√≠an que ser iguales porque las medias de los\r\nniveles del tratamiento se nivelar√≠an a la misma altura. Si las medias\r\nson significativamente distintas ¬øcual varianza ser√≠a mayor? la\r\ncalculada con SCT o la calculada con SCE? Esta es la raz√≥n por la cual\r\nel ANDEVA compara medias a trav√©s de la comparaci√≥n de varianzas!!!!\r\n¬øQu√© interpretaci√≥n tiene la diferencia entre las dos sumas\r\nmencionadas arriba? Pues es precisamente la varianza explicada por el\r\nmodelo. Esta diferencia se asocia con la siguiente gr√°fica:\r\n\r\n\r\nmodelo <- lm(y~f)\r\nplot (y)\r\nabline (mean(y), 0, col = 4)\r\npoints(predict(modelo), pch = 16, col = 5)\r\nfor (i in 1:14) lines(c(i, i), c(mean(y), predict(modelo)[i]), col = 6)\r\n\r\n\r\n\r\n\r\nC.1\r\nR.2\r\n¬øQue implica el ajuste del modelo ANOVA del factor ‚Äúf‚Äù?\r\n\r\n\r\nSCT<-sum((y-mean(y))^2)\r\nSCT\r\n\r\n\r\n[1] 55.5\r\n\r\nLa pregunta es cuanto de esta variaci√≥n es explicada por diferencias\r\nentre las medias de A y B (niveles del factor F) y cuanto por el\r\nerror\r\n\r\n\r\nSCEa<-sum((y[f==\"a\"]-mean(y[f==\"a\"]))^2)\r\nSCEb<-sum((y[f==\"b\"]-mean(y[f==\"b\"]))^2)\r\n\r\n\r\n\r\nEntonces la SCE es la suma de estas dos cantidades\r\n\r\n\r\nSCE<-SCEa+SCEb\r\nSCE\r\n\r\n\r\n[1] 24\r\n\r\nFinalmente la SCA es SCT-SCE\r\n\r\n\r\nSCA<-SCT-SCE\r\nSCA\r\n\r\n\r\n[1] 31.5\r\n\r\nEntonces ya podemos llenar la tabla de ANOVA\r\n#C.2\r\nR.3\r\nAhora calculemos la F\r\n\r\n\r\n31.5/2\r\n\r\n\r\n[1] 15.75\r\n\r\ny la p\r\n\r\n\r\n1-pf(15.75,1,12)\r\n\r\n\r\n[1] 0.001864103\r\n\r\nAhora el automatico\r\n\r\n\r\nmodelo<-aov(y~f)\r\n\r\n\r\n\r\n\r\n\r\nsummary(modelo)\r\n\r\n\r\n            Df Sum Sq Mean Sq F value  Pr(>F)   \r\nf            1   31.5    31.5   15.75 0.00186 **\r\nResiduals   12   24.0     2.0                   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nboxplot(y~f,xlab=\"factor F\",ylab=\"y\")\r\n\r\n\r\n\r\n\r\n¬øCual es la conclusi√≥n? Ahora hacemos la cr√≠tica del modelo\r\n\r\n\r\npar(mfrow=c(2,2))\r\nplot(modelo)\r\n\r\n\r\n\r\n\r\ny ahora lo ultimo\r\n\r\n\r\nA<-c(6,8,5,9,7,8,6)\r\nB<-c(9,11,8,12,10,11,9)\r\nt.test(A,B)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  A and B\r\nt = -3.9686, df = 12, p-value = 0.001864\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -4.647028 -1.352972\r\nsample estimates:\r\nmean of x mean of y \r\n        7        10 \r\n\r\nsummary(modelo)\r\n\r\n\r\n            Df Sum Sq Mean Sq F value  Pr(>F)   \r\nf            1   31.5    31.5   15.75 0.00186 **\r\nResiduals   12   24.0     2.0                   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nLa anova es una ‚ÄúGeneralizaci√≥n‚Äù de t para poder comparar mas de dos\r\nmedias. En realidad F=t^2\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:40:55-06:00"
    },
    {
      "path": "Correlacion.html",
      "title": "Script Correlaci√≥n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nVamos a ver una serie de datos para ver si existe una relaci√≥n lineal\r\nentre ellos.\r\n\r\n\r\ndata<-read.table(\"twosample.txt\",header=T)\r\nattach(data)\r\ndata\r\n\r\n\r\n\r\n\r\n\r\nplot(x,y)\r\n\r\n\r\n\r\n\r\nSe acuerdan que necesitamos primero para calcular el coeficiente de\r\ncorrelaci√≥n de pearson? Las varianzas individuales\r\n\r\n\r\nvar(x)\r\n\r\n\r\n[1] 199.9837\r\n\r\nvar(y)\r\n\r\n\r\n[1] 977.0153\r\n\r\n¬øy que m√°s? La covarianza y estamos hechos\r\n\r\n\r\nvar(x,y)\r\n\r\n\r\n[1] 414.9603\r\n\r\nAhora calculamos r\r\n\r\n\r\nvar(x,y)/sqrt(var(x)*var(y))\r\n\r\n\r\n[1] 0.9387684\r\n\r\nAhora hagamoslo en autom√°tico\r\n\r\n\r\ncor(x,y)\r\n\r\n\r\n[1] 0.9387684\r\n\r\nY ahora hagamos la prueba de hip√≥tesis\r\nCalculamos EE de r\r\n\r\n\r\nEEr<-((1-(cor(x,y)^2))/(length(x)-2))^0.5\r\nEEr\r\n\r\n\r\n[1] 0.05025759\r\n\r\nCalculo t de la muestra\r\n\r\n\r\nte<-cor(x,y)/EEr\r\nte\r\n\r\n\r\n[1] 18.67914\r\n\r\nCalculo t de tablas\r\n\r\n\r\nqt(0.975,47)\r\n\r\n\r\n[1] 2.011741\r\n\r\nCalculo la p\r\n\r\n\r\n2*(1-pt(18.67914,47))\r\n\r\n\r\n[1] 0\r\n\r\nAhora hagamoslo de manera autom√°tica\r\n\r\n\r\npearson<-cor.test(x,y)\r\npearson\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  x and y\r\nt = 18.679, df = 47, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.8934139 0.9651786\r\nsample estimates:\r\n      cor \r\n0.9387684 \r\n\r\n¬øQue nos falta?. Pues no sabemos si cumplimos con los supuestos.\r\nVeamos el de normalidad\r\n\r\n\r\npar(mfrow=c(1,2))\r\nqqnorm(x, main=\"Q-Q plot x\"); qqline(x, col = 2, lty = 2)\r\nqqnorm(y, main=\"Q-Q plot y\"); qqline(y, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\n¬øQue opciones tengo?.\r\nHacer una prueba de sesgo y kurtosis para ver si estas\r\ndesviaciones son significativas\r\nSi son significativas, puedo intentar transformaciones o puedo\r\nutilizar muchas de las otras pruebas de correlaci√≥n que son robustas a\r\nla violaci√≥n de este supuesto. Vean Q y k p.76 y Crawley\r\np.97-102.\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:39:05-06:00"
    },
    {
      "path": "Estimacion1.html",
      "title": "Script Estimaci√≥n 1 - El papel de la probabilidad",
      "author": [],
      "contents": "\r\n\r\nContents\r\nEl teorema de tendencia\r\ncentral\r\nR.1\r\nC.1\r\n\r\nR.2\r\nC.2\r\n\r\nR.3\r\nTarea de hoy\r\n¬°Fin!\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nEl teorema de tendencia\r\ncentral\r\nR.1\r\nPid√°mosle a R que tome 10000 n√∫meros al azar (0-10) y que grafique\r\ncon que frecuencia eligi√≥ cada n√∫mero ¬øque esperan ver?\r\n\r\n\r\nnumeros<-(runif(10000)*10)\r\n\r\n\r\n\r\n\r\n\r\nhist(runif(10000)*10,main=\"\")\r\n\r\n\r\n\r\n\r\nTomemos 5 n√∫meros al azar del 0 al 10 y saquemos el promedio de los\r\ncinco n√∫meros ¬øcual creen que sea el promedio t√≠pico? Hag√°monos\r\n\r\n\r\nclase<-c((mean(,,,,)), (mean(,,,,)))\r\nclase\r\n\r\n\r\n\r\nAhora pid√°mosle a R que lo haga 10000 veces (¬°10000 muestras de\r\nn√∫meros del 0 al 10!)\r\n\r\n\r\nmeans<-numeric(10000)\r\nfor (i in 0:10000){ \r\n  means[i]<- mean(runif(5)*10)}\r\n  \r\nhist(means,ylim=c(0,1600))\r\n\r\n\r\n\r\n\r\nNoten que existe una tendencia central. Lo que significa que el mero\r\nhecho de muestrear genera una tendencia central en el valor\r\npromedio.\r\nC.1\r\nR.2\r\nEl histograma de los n√∫meros se ve como una distribuci√≥n normal.\r\n¬øporque no dibujamos una funci√≥n normal sobre este histograma para ver\r\nque tan normal es? ¬øQue necesitamos saber?\r\n\r\n\r\nmean(means)\r\n\r\n\r\n[1] 4.992107\r\n\r\nsd(means)\r\n\r\n\r\n[1] 1.3046\r\n\r\ny meterlo en la ecuaci√≥n \\[ f(y) =\r\n\\frac{1/}{(\\sqrt{2 \\pi} \\sigma)} e^{y- \\mu / 2 \\sigma^2} \\] para\r\ncada valor de means verdad? R lo hace por ustedes\r\nPrimero generamos una serie de n√∫meros para el eje x que le permita a\r\nR saber cada cuando poner un puntito de la l√≠nea. Si queremos una linea\r\nsuave una buena regla es 100 n√∫meros. Como queremos n√∫meros entre 0 y\r\n10:\r\n\r\n\r\nxv<-seq(0,10,0.1)\r\n\r\n\r\n\r\nComo la funci√≥n normal tiene un √°rea bajo la curva de 1 y nosotros\r\ntenemos 10000 valores, necesitamos escalar la curva multiplic√°ndola por\r\nel n√∫mero de valores que se encuentran de cada lado de el valor de en\r\nmedio (mediana).\r\n\r\n\r\nyv<-dnorm(xv,mean=4.993275,sd=1.291482)*5000\r\n\r\n\r\n\r\n\r\n\r\nhist(means,ylim=c(0,1600))\r\nlines(xv,yv)\r\n\r\n\r\n\r\n\r\nEl ajuste es perfecto! Lo interesante del teorema de tendencia\r\ncentral es que no importa cual sea la distribuci√≥n real, si se toma una\r\nmuestra de ella, se comportar√° normalmente.\r\nC.2\r\nR.3\r\nEl estandarizar la curva normal tiene muchas ventajas. Nos permite\r\npor ejemplo saber cual es el √°rea hasta cualquier valor del eje X\r\n(llamados desviaciones est√°ndar).\r\nPid√°mosle a R que nos dibuje una Distribuci√≥n Normal Estandarizada.\r\nRecuerden que lo primero que hay que hacer cuando se dibujan l√≠neas\r\n\r\n\r\nnd<-seq(-3,3,0.01)\r\n\r\n\r\n\r\nCuando no le damos la media y a la var, significa que\r\nqueremos la estandarizada (el default), solo le dimos las\r\nx.\r\n\r\n\r\ny<-dnorm(nd)\r\nplot(nd,y,type=\"l\")\r\n\r\n\r\n\r\n\r\nAhora utilicemos pnorm para determinar que proporci√≥n de\r\nvalores caen por debajo de dos desviaciones est√°ndar.\r\n\r\n\r\npnorm(-2)\r\n\r\n\r\n[1] 0.02275013\r\n\r\nque proporci√≥n? y ahora por debajo de 1 ds\r\n\r\n\r\npnorm(-1)\r\n\r\n\r\n[1] 0.1586553\r\n\r\n¬øQue pasa si queremos saber que proporci√≥n de muestras caen A\r\nLA DERECHA de la desviaci√≥n 3?\r\nesta es una opci√≥n\r\n\r\n\r\n 1-pnorm(3)\r\n\r\n\r\n[1] 0.001349898\r\n\r\n¬øy la otra?\r\n\r\n\r\npnorm(-3)\r\n\r\n\r\n[1] 0.001349898\r\n\r\nEs claro que un valor de 3 cuando la media es 0 en una dn es muy poco\r\nprobable (0.13% de las veces cuando se saca una muestra de la pob.)\r\nAhora veamos un uso muy com√∫n de la distribuci√≥n de z. Este es\r\npreguntarse bajo una situaci√≥n aleatoria (solo por el hecho de muestrear\r\nuna poblaci√≥n) entre que desviaciones est√°ndar podemos encontrar el 95%\r\nde las muestras. (noten que al ser sim√©trica, tenemos que alojar 2.5% de\r\nlos casos ‚Äúexcedentes‚Äù de cada lado y para ello definimos un vector\r\ndentro de la funci√≥ qnorm (cuantiles normales)\r\n\r\n\r\nqnorm(c(0.025,0.975))\r\n\r\n\r\n[1] -1.959964  1.959964\r\n\r\nahora dibuj√©moslo\r\n\r\n\r\ny<-dnorm(nd)\r\n\r\nplot(nd,y,type=\"l\")\r\n  abline(v=-1.96)\r\n  abline(v=+1.96)\r\n\r\n\r\n\r\n\r\nEsto es muy importante porque si encontramos que nuestro muestreo no\r\nse ajusta a estos valores esperados entonces posiblemente pertenezcan a\r\ndos poblaciones distintas. Este es el principio b√°sico de todas las\r\npruebas de significancia (p=0.05), el origen de el error est√°ndar (1.96\r\ndesv. est√°ndar), los l√≠mites de confianza etc.\r\nTarea de hoy\r\nSupongamos que tenemos una muestra de 100 colibr√≠es de una especie A.\r\nLes hemos medido la extensi√≥n de alas y encontramos que la media es 17\r\ncm y la desviaci√≥n est√°ndar es de 0.8 cm.\r\nDibujen la distribuci√≥n de probabilidad bajo un supuesto de\r\nnormalidad\r\n¬øque probabilidad hay de encontrar un caso con una extensi√≥n\r\nmayor a 18cm?\r\nNoten que cualquier valor de y en una distribuci√≥n normal se puede\r\nconvertir en un valor de z. Recuerden que \\(z=(y-media(y))/desv.stand\\).\r\n¬øy la de encontrar un ave con una extensi√≥n menor a 15?\r\n¬øustedes creen que un ave con una extensi√≥n de 15cm puede decirse\r\nque con una confianza del 95% pertenece a la misma poblaci√≥n?\r\n¬øentre que medidas de extensi√≥n de alas se puede decir con un 95%\r\nde confianza que las aves pertenecen a esa poblaci√≥n?\r\n¬°Fin!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:14:25-06:00"
    },
    {
      "path": "Estimacion2.html",
      "title": "Script Estimaci√≥n 2",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nEstimaci√≥n de\r\npar√°metros de tendencia central\r\n\r\nC.1\r\nR.2\r\nC.2\r\nR.3\r\nMedidas de dispersi√≥n\r\nC.3\r\nR.4\r\nC.4\r\nR.5\r\nC.5\r\nR.6\r\nC.6\r\nFin ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar yvalues\r\n\r\n\r\nR.1\r\nEstimaci√≥n de\r\npar√°metros de tendencia central\r\nSabemos que aunque los datos no se agrupen alrededor de un valor\r\nt√≠pico si hacemos muestras consecutivas los estad√≠sticos de estas van a\r\ntener una tendencia central.\r\nVeamos un cuerpo de datos. Una variable y.\r\n\r\n\r\nyvals <- read.table(\"yvalues.txt\",header=T)\r\nattach(yvals)\r\nyvals\r\n\r\n\r\n\r\nUna manera muy simple de medir la tendencia central es ver cual es el\r\nvalor m√°s frecuente. Este se denomina MODA\r\n\r\n\r\nyord<-sort(y)\r\nyord\r\n\r\n\r\n [1] 1.029680 1.032829 1.037292 1.053150 1.053251 1.063707 1.064455\r\n [8] 1.070355 1.071217 1.075693 1.081053 1.084013 1.086288 1.094628\r\n[15] 1.095193 1.100543 1.101244 1.102617 1.104005 1.108847 1.108853\r\n[22] 1.109046 1.110171 1.112230 1.117421 1.120611 1.121111 1.126974\r\n[29] 1.127780 1.128891 1.130020 1.130521 1.134060 1.145682 1.149131\r\n[36] 1.150670 1.154751 1.156906 1.190229\r\n\r\n\r\n\r\n#windows()\r\nhist(y)\r\n\r\n\r\n\r\n\r\n¬øcual es la clase modal aqu√≠?\r\nPero ahora queremos saber la media aritm√©tica (el promedio) que es la\r\nsuma de todos los valores dividido por n.¬†¬øQue hago?\r\n\r\n\r\ntotal<-sum(y)\r\nsum(y)\r\n\r\n\r\n[1] 43.03511\r\n\r\npero ahora necesito saber cuantos valores son\r\n\r\n\r\nn<-length(y)\r\nn\r\n\r\n\r\n[1] 39\r\n\r\nmedia<- total/n\r\nmedia\r\n\r\n\r\n[1] 1.103464\r\n\r\npero ahora quiero tener una funci√≥n que me sirva para siempre\r\n\r\n\r\nmedia.aritmetica <- function(x) {\r\n  sum(x)/length(x) }\r\n\r\n\r\n\r\nya est√°, ahora prob√©mosla\r\n\r\n\r\ndata<-c(3,4,6,7)\r\nmedia.aritmetica(data)\r\n\r\n\r\n[1] 5\r\n\r\n\r\n\r\nmedia.aritmetica(y)\r\n\r\n\r\n[1] 1.103464\r\n\r\n¬°Que bien! ¬°R es fant√°stico! puedo calcular la media siempre que me\r\nplazca.\r\nEn realidad la mayor parte de las funciones estad√≠sticas est√°n ya\r\nconstruidas en R. Por supuesto que la media es una de ellas. Solo quer√≠a\r\nmostrarles que no hay nada obscuro detr√°s de los objetos ya creados en\r\nR\r\n\r\n\r\nmean(y)\r\n\r\n\r\n[1] 1.103464\r\n\r\nLa media como medida de tendencia central tiene el serio problema de\r\nque es muy sensible a valores at√≠picos. vean lo siguiente.\r\n\r\n\r\ndataat<-c(data,100)\r\ndataat\r\n\r\n\r\n[1]   3   4   6   7 100\r\n\r\nmean(dataat)\r\n\r\n\r\n[1] 24\r\n\r\ncomparado con\r\n\r\n\r\nmean(data)\r\n\r\n\r\n[1] 5\r\n\r\nUna alternativa es la mediana, que es el valor de en medio, una vez\r\nque todos los valores han sido ordenados. Veamos dataat\r\n\r\n\r\ndataat\r\n\r\n\r\n[1]   3   4   6   7 100\r\n\r\n¬øCual es la mediana?\r\n\r\n\r\nmedian(dataat)\r\n\r\n\r\n[1] 6\r\n\r\nes mucho mejor estimaci√≥n de el centro que 24.\r\n¬øy de data?\r\n\r\n\r\ndata\r\n\r\n\r\n[1] 3 4 6 7\r\n\r\n\r\n\r\nmedian(data)\r\n\r\n\r\n[1] 5\r\n\r\n¬ø y para y?\r\n\r\n\r\nmedian(y)\r\n\r\n\r\n[1] 1.108847\r\n\r\nmean(y)\r\n\r\n\r\n[1] 1.103464\r\n\r\nSe parecen mucho porque no hay valores at√≠picos y porque la\r\ndistribuci√≥n es sim√©trica.\r\nAhora pensemos en fen√≥menos que cambian multiplicativamente. ¬øConocen\r\nalguno?\r\nUno de los m√°s comunes en ecolog√≠a es el crecimiento poblacional y\r\npor lo tanto la dispersi√≥n de organismos de una poblaci√≥n. En dichos\r\ncasos la media aritm√©tica y/o la mediana suelen ser p√©simos estimadores\r\nde la tendencia central. Veamos un ejemplo.\r\nEl n√∫mero de insectos en una serie de plantas vecinas es\r\n\r\n\r\ninsectos<-c(1,10,1000,10,1)\r\n\r\n\r\n\r\n¬øCual es la mejor estimaci√≥n de tendencia central?\r\n\r\n\r\n#windows()\r\nhist(insectos)\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean(insectos)\r\n\r\n\r\n[1] 204.4\r\n\r\nmedian(insectos)\r\n\r\n\r\n[1] 10\r\n\r\nC.1\r\nR.2\r\nLo que se usa es la media geom√©trica que si se acuerdan hay dos\r\nmaneras de calcularla. Para el ejemplo de los insectos ¬øcual es la mas\r\nsimple?\r\n\r\n\r\n100000^0.2\r\n\r\n\r\n[1] 10\r\n\r\n¬øy la otra?\r\n\r\n\r\nexp(mean(log(insectos)))\r\n\r\n\r\n[1] 10\r\n\r\n\r\n\r\ndetach(yvals)\r\nrm(insectos)\r\nls()\r\n\r\n\r\n[1] \"data\"             \"dataat\"           \"media\"           \r\n[4] \"media.aritmetica\" \"n\"                \"total\"           \r\n[7] \"yord\"             \"yvals\"           \r\n\r\nC.2\r\nR.3\r\nMedidas de dispersi√≥n\r\nVeamos un cuerpo de datos cualquiera y pregunt√©monos c√≥mo podemos\r\nmedir su dispersi√≥n.\r\n\r\n\r\ny<-c(13,7,5,12,9,15,6,11,9,7,12)\r\n\r\n\r\n\r\nveamos c√≥mo se ve\r\n\r\n\r\n#windows()\r\nplot(y,ylim=c(0,20))\r\n\r\n\r\n\r\n\r\nLo m√°s f√°cil es decir de donde a donde va (el intervalo).\r\n\r\n\r\nrange(y)\r\n\r\n\r\n[1]  5 15\r\n\r\nPero esto tiene sus problemas.\r\nNo tiene relaci√≥n con el par√°metro poblaci√≥n de intervalo.\r\nIncrementa con la n.\r\nAdem√°s es muy susceptible a valores at√≠picos\r\nNo considera a todos lo valores.\r\nOtra medida de dispersi√≥n muy importante es la varianza. Que est√°\r\nfundamentada en las desviaciones (o residuales) de cada valor con la\r\nmedia\r\n\r\n\r\ny<-c(13,7,5,12,9,15,6,11,9,7,12)\r\nplot(y,ylim=c(0,20))\r\nabline(mean(y),0)\r\nfor (i in 1:11) lines(c(i,i),c(y[i],mean(y)))\r\n\r\n\r\n\r\n\r\nse usa la suma de cuadrados de la diferencia de cada valor con la\r\nmedia general como base. ¬øC√≥mo lo calculamos?\r\n\r\n\r\ny-mean(y)\r\n\r\n\r\n [1]  3.3636364 -2.6363636 -4.6363636  2.3636364 -0.6363636  5.3636364\r\n [7] -3.6363636  1.3636364 -0.6363636 -2.6363636  2.3636364\r\n\r\n(y-mean(y))^2\r\n\r\n\r\n [1] 11.3140496  6.9504132 21.4958678  5.5867769  0.4049587 28.7685950\r\n [7] 13.2231405  1.8595041  0.4049587  6.9504132  5.5867769\r\n\r\nsum((y-mean(y))^2)\r\n\r\n\r\n[1] 102.5455\r\n\r\nFant√°sitico, pero que sucede a SC cada vez que yo adiciono una nueva\r\nobservaci√≥n. ¬øQue tenemos que hacer?\r\nSi divido entre n, se llama la desviaci√≥n media de los cuadrados.\r\nC.3\r\nR.4\r\n\r\n\r\nvariance <- function (x)   sum((x-mean(x))^2)/(length(x)-1)\r\nvariance(y)\r\n\r\n\r\n[1] 10.25455\r\n\r\nPero claro, ya est√° definido.\r\n\r\n\r\nvar(y)\r\n\r\n\r\n[1] 10.25455\r\n\r\nLa relaci√≥n entre la varianza de la muestra y el tama√±o de muestra\r\n(n)\r\nLo que vamos a hacer es seleccionar aleatoriamente n√∫meros de una\r\npoblaci√≥n que tiene una distribuci√≥n normal (media 10 y var 4). Esto lo\r\nvamos a hacer repetidas veces pero nuestra muestra va a ir incrementando\r\nsu n desde 3 hasta 31. Vamos a sacar 30 muestras de cada tama√±o de\r\nmuestra. Es decir 30 muestras de 3 n√∫meros, 30 muestras de 5 n√∫meros\r\netc. A cada muestra le vamos a calcular su varianza y las vamos a\r\ngraficar.\r\n\r\n\r\n# windows()\r\nplot(c(0,32),c(0,15),type=\"n\",xlab=\"Tama√±o de muestra\",ylab=\"Varianza\")\r\nfor (tm in seq(3,31,2)) {\r\nfor( i in 1:30){\r\nx<-rnorm(tm,mean=10,sd=2)\r\npoints(tm,var(x)) }}\r\n\r\n\r\n\r\n\r\nAhora pueden ver que la varianza poblacional puede estar muy mal\r\nestimada con tama√±os de muestra peque√±os. Y a medida que aumentamos el\r\ntama√±o de muestra la probabilidad de que la estimaci√≥n est√° muy lejos\r\ndel par√°metro disminuye. Esta es una raz√≥n m√°s para elegir muy\r\ncuidadosamente el tama√±o de muestra. En unas clases vamos a hablar de\r\nesto en el contexto de pruebas de hip√≥tesis.\r\nC.4\r\nR.5\r\nHagamos el mismo proceso que hicimos antes para elegir muestras con\r\ntama√±os de muestra que incrementan pero ahora veamos que sucede con\r\nnuestra medida de desconfianza de la estimaci√≥n a medida que aumenta\r\nn\r\n\r\n\r\n#windows()\r\nplot(c(0,32),c(0,2),type=\"n\",xlab=\"Tama√±o de muestra\",ylab=\"Error estandar\")\r\n for (tm in seq(3,31,2)) {\r\nfor( i in 1:30){\r\nx<-rnorm(tm,mean=10,sd=2)\r\npoints(tm,sqrt(var(x)/tm)) }}\r\n\r\n\r\n\r\n\r\nVeamos un ejemplo de la concentraci√≥n de ozono en unos\r\ninvernaderos\r\n\r\n\r\nozono<- read.table(\"gardens.txt\",header=T)\r\nattach(ozono)\r\nozono\r\n\r\n\r\n\r\n\r\n\r\n#windows()\r\npar(mfrow=c(1,3))\r\nplot (gardenA)\r\nplot (gardenB)\r\nplot (gardenC)\r\n\r\n\r\n\r\n\r\n\r\n\r\nMediaA<- mean(gardenA)\r\nEEA<- sqrt(var(gardenA)/10)\r\nMediaB<- mean(gardenB)\r\nEEB<- sqrt(var(gardenB)/10)\r\nMediaC<- mean(gardenC)\r\nEEC<- sqrt(var(gardenC)/10)\r\n\r\nMediaA\r\n\r\n\r\n[1] 3\r\n\r\nEEA\r\n\r\n\r\n[1] 0.3651484\r\n\r\nMediaB\r\n\r\n\r\n[1] 5\r\n\r\nEEB\r\n\r\n\r\n[1] 0.3651484\r\n\r\nMediaC\r\n\r\n\r\n[1] 5\r\n\r\nEEC\r\n\r\n\r\n[1] 1.19257\r\n\r\n¬øPara cual invernadero puedo yo confiar m√°s de la estimaci√≥n de la\r\nmedia?\r\nC.5\r\nR.6\r\nVamos a calcular los intervalos de confianza para la media de los\r\ninvernaderos usando la distribuci√≥n de t. qt nos d? el valor de t para\r\nel cual hay cierta proporci√≥n a la izquierda.\r\n\r\n\r\nqt(.975,9)\r\n\r\n\r\n[1] 2.262157\r\n\r\ncalculemos los intervalos de confianza al 95% para el invernadero\r\nB\r\n\r\n\r\nqt(.975,9)*sqrt(1.3333/10)\r\n\r\n\r\n[1] 0.8260127\r\n\r\nel reporte diario, la concentraci√≥n promedio de ozono en el\r\ninvernadero B fue de 5.0?0.826(I.C.95%, n=10)\r\n\r\n\r\n#windows()\r\nplot(gardenB)\r\nabline(mean(gardenB),0)\r\nabline((mean(gardenB)+0.826),0, lty=2)\r\nabline((mean(gardenB)-0.826),0, lty=2)\r\n\r\n\r\n\r\n\r\nlos dejo que ustedes calculen aquellos de los invernaderos A y C.\r\nC.6\r\nFin ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:04:55-06:00"
    },
    {
      "path": "Estimacion2bis.html",
      "title": "Script estimacion 2_bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntervalos de confianza\r\nC.6\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nIntervalos de confianza\r\nExiste otra manera enteramente distinta de estimar intervalos de\r\nconfianza, y esta es enteramente autoreferida. Se usa el m√©todo de\r\nremuestreo o bootstrapping. Veamos un cuerpo de datos.\r\n\r\n\r\ndata<-read.table(\"skewdata.txt\",header=T)\r\nattach(data)\r\nnames(data)\r\n\r\n\r\n\r\nvamos a verlos visual y gr√°ficamente\r\n\r\n\r\nvalues\r\n\r\n\r\n [1] 81.372918 25.700971  4.942646 43.020853 81.690589 51.195236\r\n [7] 55.659909 15.153155 38.745780 12.610385 22.415094 18.355721\r\n[13] 38.081501 48.171135 18.462725 44.642251 25.391082 20.410874\r\n[19] 15.778187 19.351485 20.189991 27.795406 25.268600 20.177459\r\n[25] 15.196887 26.206537 19.190966 35.481161 28.094252 30.305922\r\n\r\n\r\n\r\nhist(values)\r\n\r\n\r\n\r\n\r\nAhora vamos a calcular los intervalos de confianza de remuestreo por\r\nquantiles a trav√©s de la funci√≥n sample que remuestrea y la funci√≥n\r\nquantile basados en la muestra. Ahora, la funci√≥n construida creo que no\r\nexiste, as√≠ que aqu√≠ la definimos.\r\n\r\n\r\nRemuestreo<-function(x){\r\na<-numeric(10000)\r\nfor (i in 1:10000){\r\na[i]<-mean(sample(x,30,replace=T))}\r\nquantile(a,c(.025,.975))}\r\nRemuestreo(values)\r\n\r\n\r\n    2.5%    97.5% \r\n24.89675 37.86573 \r\n\r\nVeamos como comparan con los IC normales\r\n\r\n\r\nmean(values)+1.96*sqrt(var(values)/30)\r\n\r\n\r\n[1] 37.53846\r\n\r\nmean(values)-1.96*sqrt(var(values)/30) \r\n\r\n\r\n[1] 24.39885\r\n\r\nVean ustedes porque elegimos 30 como una n buena para tama√±o de las\r\nmuestras repetidas\r\n\r\n\r\nplot(c(0,60),c(0,60),type=\"n\",xlab=\"Tama√±o de muestra\",ylab=\"IC remuestreo\")\r\n \r\nfor (k in seq(5,60,3)){\r\na<-numeric(10000)\r\nfor (i in 1:10000){\r\na[i]<-mean(sample(values,k,replace=T))\r\n}\r\npoints(c(k,k),quantile(a,c(.025,.975)),type=\"b\")\r\n}\r\n\r\n# Ahora veamos como comparan con los valores de IC normales\r\nxv<-seq(5,60,0.1)\r\nyv<-mean(values)+1.96*sqrt(var(values)/xv)\r\nlines(xv,yv)\r\nyv<-mean(values)-1.96*sqrt(var(values)/xv)\r\nlines(xv,yv)\r\n\r\n#y los valores de IC de t student \r\n\r\nyv<-mean(values)-qt(.975,xv)*sqrt(var(values)/xv)\r\nlines(xv,yv,lty=2)\r\nyv<-mean(values)+qt(.975,xv)*sqrt(var(values)/xv)\r\nlines(xv,yv,lty=2)\r\n\r\n\r\n\r\n\r\nNoten que no son exactamente sim√©tricos. Esto hace que sean\r\nligeramente sesgados. Existen otros dos m√©todos (por centiles y\r\nacelerados) que corrigen esto. Tambi√©n noten que para la distribuci√≥n de\r\nt son m√°s conservadores si el tama√±o de muestra es peque√±o.\r\nC.6\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:20:08-06:00"
    },
    {
      "path": "Exploratorios.html",
      "title": "Script An√°lisis exploratorios",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nExploraci√≥n Univariada\r\nRelaciones bivariadas\r\nC.1\r\nR.2\r\nvalores faltantes\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el csv donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nESTE EJEMPLO ES DE DATOS DE EVERITT 04 P. 17. CONSISTE EN\r\nINFORMACI√ìN SOBRE CONTAMINACI√ìN AMBIENTAL EN EU EN ZONAS\r\nMETROPOLITANAS.\r\nLlamo los datos (ojo que Everitt los tiene en formato dat, si est√°n\r\ncomo txt, hay que llamarlos con read.table)\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\nap <- data.frame(airpoll)\r\n# write.csv(ap, \"apdf.csv\")\r\nadf <- read.csv(\"airpoll.csv\", header = T, sep = \";\") #tabla con dato faltante\r\n\r\n\r\n\r\n\r\n\r\nattach(airpoll)\r\nairpoll\r\n\r\n\r\n         Rainfall Education Popden Nonwhite NOX SO2 Mortality\r\nakronOH        36      11.4   3243      8.8  15  59     921.9\r\nalbanyNY       35      11.0   4281      3.5  10  39     997.9\r\nallenPA        44       9.8   4260      0.8   6  33     962.4\r\natlantGA       47      11.1   3125     27.1   8  24     982.3\r\nbaltimMD       43       9.6   6441     24.4  38 206    1071.0\r\nbirmhmAL       53      10.2   3325     38.5  32  72    1030.0\r\nbostonMA       43      12.1   4679      3.5  32  62     934.7\r\nbridgeCT       45      10.6   2140      5.3   4   4     899.5\r\nbufaloNY       36      10.5   6582      8.1  12  37    1002.0\r\ncantonOH       36      10.7   4213      6.7   7  20     912.3\r\nchatagTN       52       9.6   2302     22.2   8  27    1018.0\r\nchicagIL       33      10.9   6122     16.3  63 278    1025.0\r\ncinnciOH       40      10.2   4101     13.0  26 146     970.5\r\nclevelOH       35      11.1   3042     14.7  21  64     986.0\r\ncolombOH       37      11.9   4259     13.1   9  15     958.8\r\ndallasTX       35      11.8   1441     14.8   1   1     860.1\r\ndaytonOH       36      11.4   4029     12.4   4  16     936.2\r\ndenverCO       15      12.2   4824      4.7   8  28     871.8\r\ndetrotMI       31      10.8   4834     15.8  35 124     959.2\r\nflintMI        30      10.8   3694     13.1   4  11     941.2\r\nftwortTX       31      11.4   1844     11.5   1   1     891.7\r\ngrndraMI       31      10.9   3226      5.1   3  10     871.3\r\ngrnborNC       42      10.4   2269     22.7   3   5     971.1\r\nhartfdCT       43      11.5   2909      7.2   3  10     887.5\r\nhoustnTX       46      11.4   2647     21.0   5   1     952.5\r\nindianIN       39      11.4   4412     15.6   7  33     968.7\r\nkansasMO       35      12.0   3262     12.6   4   4     919.7\r\nlancasPA       43       9.5   3214      2.9   7  32     844.1\r\nlosangCA       11      12.1   4700      7.8 319 130     861.8\r\nlouisvKY       30       9.9   4474     13.1  37 193     989.3\r\nmemphsTN       50      10.4   3497     36.7  18  34    1006.0\r\nmiamiFL        60      11.5   4657     13.5   1   1     861.4\r\nmilwauWI       30      11.1   2934      5.8  23 125     929.2\r\nminnplMN       25      12.1   2095      2.0  11  26     857.6\r\nnashvlTN       45      10.1   2082     21.0  14  78     961.0\r\nnewhvnCT       46      11.3   3327      8.8   3   8     923.2\r\nneworlLA       54       9.7   3172     31.4  17   1    1113.0\r\nnewyrkNY       42      10.7   7462     11.3  26 108     994.6\r\nphiladPA       42      10.5   6092     17.5  32 161    1015.0\r\npittsbPA       36      10.6   3437      8.1  59 263     991.3\r\nportldOR       37      12.0   3387      3.6  21  44     894.0\r\nprovdcRI       42      10.1   3508      2.2   4  18     938.5\r\nreadngPA       41       9.6   4843      2.7  11  89     946.2\r\nrichmdVA       44      11.0   3768     28.6   9  48    1026.0\r\nrochtrNY       32      11.1   4355      5.0   4  18     874.3\r\nstlousMO       34       9.7   5160     17.2  15  68     953.6\r\nsandigCA       10      12.1   3033      5.9  66  20     839.7\r\nsanfrnCA       18      12.2   4253     13.7 171  86     911.7\r\nsanjosCA       13      12.2   2702      3.0  32   3     790.7\r\nseatleWA       35      12.2   3626      5.7   7  20     899.3\r\nspringMA       45      11.1   1883      3.4   4  20     904.2\r\nsyracuNY       38      11.4   4923      3.8   5  25     950.7\r\ntoledoOH       31      10.7   3249      9.5   7  25     972.5\r\nuticaNY        40      10.3   1671      2.5   2  11     912.2\r\nwashDC         41      12.3   5308     25.9  28 102     968.8\r\nwichtaKS       28      12.1   3665      7.5   2   1     823.8\r\nwilmtnDE       45      11.3   3152     12.1  11  42    1004.0\r\nworctrMA       45      11.1   3678      1.0   3   8     895.7\r\nyorkPA         42       9.0   9699      4.8   8  49     911.8\r\nyoungsOH       38      10.7   3451     11.7  13  39     954.4\r\n\r\nnames(airpoll)\r\n\r\n\r\n[1] \"Rainfall\"  \"Education\" \"Popden\"    \"Nonwhite\"  \"NOX\"      \r\n[6] \"SO2\"       \"Mortality\"\r\n\r\nExploraci√≥n Univariada\r\nComenzamos por ver el vector de medias y varianzas\r\n#mean(airpoll) #sd(airpoll)^2\r\n\r\n\r\nsummary(airpoll)\r\n\r\n\r\n    Rainfall       Education         Popden        Nonwhite    \r\n Min.   :10.00   Min.   : 9.00   Min.   :1441   Min.   : 0.80  \r\n 1st Qu.:32.75   1st Qu.:10.40   1st Qu.:3104   1st Qu.: 4.95  \r\n Median :38.00   Median :11.05   Median :3567   Median :10.40  \r\n Mean   :37.37   Mean   :10.97   Mean   :3866   Mean   :11.87  \r\n 3rd Qu.:43.25   3rd Qu.:11.50   3rd Qu.:4520   3rd Qu.:15.65  \r\n Max.   :60.00   Max.   :12.30   Max.   :9699   Max.   :38.50  \r\n      NOX              SO2           Mortality     \r\n Min.   :  1.00   Min.   :  1.00   Min.   : 790.7  \r\n 1st Qu.:  4.00   1st Qu.: 11.00   1st Qu.: 898.4  \r\n Median :  9.00   Median : 30.00   Median : 943.7  \r\n Mean   : 22.65   Mean   : 53.77   Mean   : 940.4  \r\n 3rd Qu.: 23.75   3rd Qu.: 69.00   3rd Qu.: 983.2  \r\n Max.   :319.00   Max.   :278.00   Max.   :1113.0  \r\n\r\n\r\n\r\nsummary(airpoll$SO2)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   1.00   11.00   30.00   53.77   69.00  278.00 \r\n\r\nvease la diferencia entre la media y la mediana para reconocer\r\ndesviaciones, calculese el intervalo intercuartiles (3er-1er).\r\n\r\n\r\n#windows()\r\nboxplot(SO2, range=0, ylab=\"SO2\") # en este caso, los \"bigotes\" del boxplot ubican el m√°ximo (1) y el m√≠nimo (278).\r\n\r\n\r\n\r\n\r\n\r\n\r\nboxplot(SO2, ylab=\"SO2\") #en este caso, la funci√≥n se ejecuta con range = 1.5 por defecto.\r\n\r\n\r\n\r\n\r\n\r\n\r\niqSO2<-69-11\r\niqSO2\r\n\r\n\r\n[1] 58\r\n\r\nUna buena regla de dedo para identificar datos at√≠picos es: que los\r\npuntos que caen mas all√° del 3er+1.5(intercuartil) o mas bajo que\r\n1er-1.5(intercuartil) son valores at√≠picos.\r\n\r\n\r\natipicossup<-69+(iqSO2*1.5)\r\n  atipicossup\r\n\r\n\r\n[1] 156\r\n\r\n\r\n\r\natipicosinf<-abs(11-(iqSO2*1.5))\r\n  atipicosinf\r\n\r\n\r\n[1] 76\r\n\r\n\r\n\r\nhist(SO2,lwd=2)\r\nabline(v = 156, col = \"blue\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nairpoll\r\n\r\n\r\n         Rainfall Education Popden Nonwhite NOX SO2 Mortality\r\nakronOH        36      11.4   3243      8.8  15  59     921.9\r\nalbanyNY       35      11.0   4281      3.5  10  39     997.9\r\nallenPA        44       9.8   4260      0.8   6  33     962.4\r\natlantGA       47      11.1   3125     27.1   8  24     982.3\r\nbaltimMD       43       9.6   6441     24.4  38 206    1071.0\r\nbirmhmAL       53      10.2   3325     38.5  32  72    1030.0\r\nbostonMA       43      12.1   4679      3.5  32  62     934.7\r\nbridgeCT       45      10.6   2140      5.3   4   4     899.5\r\nbufaloNY       36      10.5   6582      8.1  12  37    1002.0\r\ncantonOH       36      10.7   4213      6.7   7  20     912.3\r\nchatagTN       52       9.6   2302     22.2   8  27    1018.0\r\nchicagIL       33      10.9   6122     16.3  63 278    1025.0\r\ncinnciOH       40      10.2   4101     13.0  26 146     970.5\r\nclevelOH       35      11.1   3042     14.7  21  64     986.0\r\ncolombOH       37      11.9   4259     13.1   9  15     958.8\r\ndallasTX       35      11.8   1441     14.8   1   1     860.1\r\ndaytonOH       36      11.4   4029     12.4   4  16     936.2\r\ndenverCO       15      12.2   4824      4.7   8  28     871.8\r\ndetrotMI       31      10.8   4834     15.8  35 124     959.2\r\nflintMI        30      10.8   3694     13.1   4  11     941.2\r\nftwortTX       31      11.4   1844     11.5   1   1     891.7\r\ngrndraMI       31      10.9   3226      5.1   3  10     871.3\r\ngrnborNC       42      10.4   2269     22.7   3   5     971.1\r\nhartfdCT       43      11.5   2909      7.2   3  10     887.5\r\nhoustnTX       46      11.4   2647     21.0   5   1     952.5\r\nindianIN       39      11.4   4412     15.6   7  33     968.7\r\nkansasMO       35      12.0   3262     12.6   4   4     919.7\r\nlancasPA       43       9.5   3214      2.9   7  32     844.1\r\nlosangCA       11      12.1   4700      7.8 319 130     861.8\r\nlouisvKY       30       9.9   4474     13.1  37 193     989.3\r\nmemphsTN       50      10.4   3497     36.7  18  34    1006.0\r\nmiamiFL        60      11.5   4657     13.5   1   1     861.4\r\nmilwauWI       30      11.1   2934      5.8  23 125     929.2\r\nminnplMN       25      12.1   2095      2.0  11  26     857.6\r\nnashvlTN       45      10.1   2082     21.0  14  78     961.0\r\nnewhvnCT       46      11.3   3327      8.8   3   8     923.2\r\nneworlLA       54       9.7   3172     31.4  17   1    1113.0\r\nnewyrkNY       42      10.7   7462     11.3  26 108     994.6\r\nphiladPA       42      10.5   6092     17.5  32 161    1015.0\r\npittsbPA       36      10.6   3437      8.1  59 263     991.3\r\nportldOR       37      12.0   3387      3.6  21  44     894.0\r\nprovdcRI       42      10.1   3508      2.2   4  18     938.5\r\nreadngPA       41       9.6   4843      2.7  11  89     946.2\r\nrichmdVA       44      11.0   3768     28.6   9  48    1026.0\r\nrochtrNY       32      11.1   4355      5.0   4  18     874.3\r\nstlousMO       34       9.7   5160     17.2  15  68     953.6\r\nsandigCA       10      12.1   3033      5.9  66  20     839.7\r\nsanfrnCA       18      12.2   4253     13.7 171  86     911.7\r\nsanjosCA       13      12.2   2702      3.0  32   3     790.7\r\nseatleWA       35      12.2   3626      5.7   7  20     899.3\r\nspringMA       45      11.1   1883      3.4   4  20     904.2\r\nsyracuNY       38      11.4   4923      3.8   5  25     950.7\r\ntoledoOH       31      10.7   3249      9.5   7  25     972.5\r\nuticaNY        40      10.3   1671      2.5   2  11     912.2\r\nwashDC         41      12.3   5308     25.9  28 102     968.8\r\nwichtaKS       28      12.1   3665      7.5   2   1     823.8\r\nwilmtnDE       45      11.3   3152     12.1  11  42    1004.0\r\nworctrMA       45      11.1   3678      1.0   3   8     895.7\r\nyorkPA         42       9.0   9699      4.8   8  49     911.8\r\nyoungsOH       38      10.7   3451     11.7  13  39     954.4\r\n\r\n¬øCuales son los valores at√≠picos para SO2? Ahora veamos lo que\r\nconsidera R como at√≠picos por default\r\n\r\n\r\npar(mfrow=c(1,3))\r\nboxplot(SO2, range=0, ylab=\"SO2\")\r\nboxplot(SO2, ylab=\"SO2\")\r\nboxplot(SO2, range=1.5, ylab=\"SO2\")\r\n\r\n\r\n\r\n\r\nVeamos las distribuciones de todas\r\n\r\n\r\npar(mfrow=c(3,3))\r\nhist(SO2,lwd=2); abline(v = c(53.77, 30), col = c(\"blue\", \"red\"))\r\nhist(Rainfall,lwd=2)\r\nhist(Education,lwd=2)\r\nhist(Popden,lwd=2)\r\nhist(Nonwhite,lwd=2)\r\nhist(NOX,lwd=2)\r\nhist(Mortality,lwd=2)\r\n\r\n\r\n\r\n\r\n¬øreconocen desviaciones negativas o positivas? Son normales?\r\n\r\n\r\npar(mfrow=c(3,3))                                                 \r\nqqnorm(SO2, main=\"Q-Q plot SO2\"); qqline(SO2, col = 2, lty = 2)\r\nqqnorm(Rainfall, main=\"Q-Q plot Rainfall\"); qqline(Rainfall, col = 2, lty = 2)\r\nqqnorm(Education, main=\"Q-Q plot Education\"); qqline(Education, col = 2, lty = 2)\r\nqqnorm(Popden, main=\"Q-Q plot Popden\"); qqline(Popden, col = 2, lty = 2)\r\nqqnorm(Nonwhite, main=\"Q-Q plot Nonwhite\"); qqline(Nonwhite, col = 2, lty = 2)\r\nqqnorm(NOX, main=\"Q-Q plot NOX\"); qqline(NOX, col = 2, lty = 2)\r\nqqnorm(Mortality, main=\"Q-Q plot Mortality\"); qqline(Mortality, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\nRelaciones bivariadas\r\nVeamos que relaci√≥n hay entre las distintas variables. Aqu√≠ utilizo\r\nuna funci√≥n smoooth (regresi√≥n con pesos locales) que permite sugerir\r\ncon los propios datos que tipo de relaci√≥n pudieran tener.\r\n\r\n\r\npairs(airpoll, panel=panel.smooth)\r\n\r\n\r\n\r\n\r\nveamos con mas detalle la relaci√≥n SO2-mortalidad\r\n\r\n\r\nnombres<-abbreviate(row.names(airpoll))\r\npar(mfrow=c(1,1))\r\nplot(SO2,Mortality,lwd=2,type=\"n\")\r\ntext(SO2,Mortality,labels=nombres,lwd=2)\r\n\r\n\r\n\r\n\r\n\r\n\r\ndetach(airpoll)\r\n\r\n\r\n\r\nC.1\r\nR.2 valores faltantes\r\n\r\n\r\nairpoldf <- read.table(\"datofalta.txt\")\r\nairpoldf\r\nattach(airpoldf)\r\n\r\n\r\n\r\nLo mas f√°cil la media\r\n\r\n\r\nsummary(Mortality)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n  789.7   892.4   943.7   936.6   977.1  1112.0       1 \r\n\r\n\r\n\r\nsum(is.na(Mortality))\r\n\r\n\r\n[1] 1\r\n\r\nCual es el valor imputado? Cuales son los problemas asociados a esta\r\nimputaci√≥n?\r\nregresi√≥n mortalidad y SO2\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nplot(SO2,Mortality,lwd=2)\r\nabline(v = 59, h = 940.2)\r\nabline(v = 59, h = 921.9, col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nregmort<-lm(Mortality~SO2)\r\nsummary(regmort)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Mortality ~ SO2)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-126.625  -38.213   -7.796   35.582  196.528 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 915.4721     9.4932  96.435  < 2e-16 ***\r\nSO2           0.4266     0.1177   3.624  0.00062 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 57.47 on 57 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.1872,    Adjusted R-squared:  0.173 \r\nF-statistic: 13.13 on 1 and 57 DF,  p-value: 0.0006196\r\n\r\nm <-  (915.4720997 + (0.4266209*59))  \r\n\r\n\r\n\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nplot(SO2,Mortality,lwd=2)\r\nabline(v = 59, h = 940.2)\r\nabline(v = 59, h = 921.9, col = \"red\")\r\nabline(lm(Mortality~SO2))\r\n\r\n\r\n\r\n\r\n\r\n\r\npredict(regmort, list(SO2=58)) \r\n\r\n\r\n       1 \r\n940.2161 \r\n\r\n\r\n\r\nlogM<-log(Mortality)\r\nlogSO2<-log(SO2+7)\r\nloglog<-lm(logM~logSO2)\r\nsummary(loglog)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = logM ~ logSO2)\r\n\r\nResiduals:\r\n      Min        1Q    Median        3Q       Max \r\n-0.136793 -0.030759  0.000398  0.029763  0.212163 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 6.749926   0.021463 314.487  < 2e-16 ***\r\nlogSO2      0.026633   0.005928   4.493 3.48e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.05863 on 57 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.2615,    Adjusted R-squared:  0.2486 \r\nF-statistic: 20.19 on 1 and 57 DF,  p-value: 3.482e-05\r\n\r\n\r\n\r\npar(mfrow=c(1,2))\r\nplot(SO2,Mortality,lwd=2) \r\nabline(regmort)\r\nabline(v = 58, h = c(940.2161, 954.4211), col = \"red\")\r\n      \r\nplot(logSO2,logM,lwd=2) \r\nabline(lm(logM~logSO2))\r\nabline(v = 58, h = 940.2161, col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nplot(logSO2,logM,lwd=2) \r\nabline(lm(logM~logSO2))\r\n\r\n\r\n\r\n\r\nel valor de SO2 que corresponde al valor faltante de mortalidad es\r\n58. Como hemos generado un modelo de logaritmos a ambos lados de la\r\necuaci√≥n sacamos el log del (SO2+7)\r\n\r\n\r\nlog(58+7)\r\n\r\n\r\n[1] 4.174387\r\n\r\nUsamos la funci√≥n predict para predecir el valor correspondiente de\r\nMortalidad\r\n\r\n\r\npredict(loglog, list(logSO2=4.174387)) \r\n\r\n\r\n       1 \r\n6.861105 \r\n\r\npero recordando que usamos logaritmos en el modelo,\r\nretrotransformamos con el antilog con base e (e elevado al numero que\r\nnos interesa retro transformar)\r\n\r\n\r\nexp(6.861105)\r\n\r\n\r\n[1] 954.4211\r\n\r\nEl valor predicho por regresi√≥n lineal es? Cuales son los problemas\r\nasociados a esta imputaci√≥n?\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:24:26-06:00"
    },
    {
      "path": "index.html",
      "title": "Introducci√≥n a la estad√≠stica inferencial",
      "description": "P√°gina del curso en su versi√≥n 2023\n",
      "author": [
        {
          "name": "Simoneta Negrete Yankelevich",
          "url": "https://www.researchgate.net/profile/Simoneta-Negrete-Yankelevich"
        },
        {
          "name": "Carlos Cultid Medina",
          "url": "https://www.researchgate.net/profile/Carlos-Cultid-Medina"
        }
      ],
      "date": "Enero 16, 2022",
      "contents": "\r\n\r\nContents\r\n¬°Bienvenidos al curso!üôå\r\nCronograma del curso üìÜ\r\nMaterial de la clase üìö\r\nScripts de\r\nclase üìä\r\nAdicional\r\n\r\n\r\n¬°Bienvenidos al curso!üôå\r\nEste es la p√°gina del curso de Introducci√≥n a la estad√≠stica\r\ninferencial, versi√≥n 2023.\r\n\r\nCronograma del curso üìÜ\r\n\r\nMaterial de la clase üìö\r\nEl material del curso est√° disponible en la p√°gina de classroom, en\r\nlas siguientes ligas\r\n√âl\r\npapel de la estad√≠stica en la investigaci√≥n\r\nIntroducci√≥n\r\nal Lenguaje R\r\nEstimaci√≥n\r\n1 y 2\r\nAn√°lisis\r\nexploratorios\r\nPrueba\r\nde hip√≥tesis 1 y 2\r\nModelos\r\nlineales 1 y 2\r\nScripts de clase üìä\r\nLos scripts pueden ser visualizados en html en la parte superior\r\nderecha de esta p√°gina.\r\n\r\nRecuerden que las bases de datos se encuentran en el material de\r\nclase\r\nAdicional\r\nMonitor del curso 2023\r\nGabriel P. Andrade Ponce üìß gabriel.andrade@posgrado.ecologia.edu.mx\r\nEdici√≥n de Scripts y p√°gina web: Gabriel Andrade\r\nPonce\r\nP√°gina hecha con la paqueter√≠a distill en RMarkdown\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:41:31-06:00"
    },
    {
      "path": "PanelCorrel.html",
      "title": "Panel de correlaci√≥n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPerformance Analyticis\r\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì\r\nCorrplot\r\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\r\nggcorrplot\r\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì\r\nGGally\r\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nPerformance Analyticis\r\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\n\r\n\r\n\r\n\r\n\r\nlibrary(PerformanceAnalytics)\r\n\r\n\r\n\r\nchart.Correlation(log(airpoll+1),\r\n                  method=\"pearson\",\r\n                  histogram=TRUE,\r\n                  pch=20)\r\n\r\n\r\n\r\n\r\n?chart.Correlation\r\nCorrplot ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\r\n\r\n\r\nlibrary(corrplot)\r\n\r\ncorr <- round(cor(log(airpoll+1), method = \"spearman\"),2)\r\ncor.mat <- cor.mtest(log(airpoll+1), conf.level = 0.95)\r\n\r\n\r\np1 <- corrplot(corr, method=\"color\",  \r\n         type=\"upper\", order=\"hclust\", \r\n         addCoef.col = \"black\", # Add coefficient of correlation\r\n         tl.col=\"black\", tl.srt=45, #Text label color and rotation\r\n         # Combine with significance\r\n         p.mat= cor.mat$p, sig.level = 0.01, insig = \"blank\", \r\n         # hide correlation coefficient on the principal diagonal\r\n         diag=FALSE \r\n)$corrPos      \r\ntext(p1$x, p1$y, round(p1$corr, 2))\r\n\r\n\r\n\r\n\r\nggcorrplot ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì\r\n\r\n\r\nlibrary(ggcorrplot)\r\n         \r\nggcorrplot(corr, \r\n           type = \"lower\",\r\n           lab = T, show.diag = F, \r\n           legend.title = \" Pearson\\nCorrelation\", \r\n           colors= c(\"#BB4444\", \"#FFFFFF\", \"#4477AA\"), \r\n           hc.order = T, \r\n           sig.level = 0.05, insig = \"pch\", pch=8, pch.cex = 2,  \r\n           p.mat= cor.mat$p, \r\n           ggtheme = ggplot2::theme(\r\n            panel.background = element_blank()))  \r\n\r\n\r\n\r\n\r\nGGally ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r\n\r\n\r\nlibrary(GGally)\r\n\r\npairs <- ggpairs(log(airpoll+1),\r\n        upper = list(continuous= wrap(\"cor\", method= \"pearson\", digits=2)),\r\n        lower = list( continuous= \"smooth\")) +theme_classic()\r\n\r\npairs\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:28:00-06:00"
    },
    {
      "path": "Phdospob.html",
      "title": "Script Prueba de hip√≥tesis de dos poblaciones",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.3\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.3\r\nVolvamos al ejemplo de las concentraciones de ozono en los\r\ninvernaderos y vamos a preguntarnos si el promedio de sus\r\nconcentraciones de ozono es significativamente distinto\r\n\r\n\r\nozono<-read.table(\"gardens.txt\",header=T)\r\nattach(ozono)\r\nnames(ozono)\r\n\r\n\r\n\r\nVeamos un gr√°fico\r\n\r\n\r\nozonoAB<-c(gardenA,gardenB)\r\nozonoAB\r\n\r\n\r\n [1] 3 4 4 3 2 3 1 3 5 2 5 5 6 7 4 4 3 5 6 5\r\n\r\netiqueta<-factor(c(rep(\"A\",10),rep(\"B\",10)))\r\netiqueta\r\n\r\n\r\n [1] A A A A A A A A A A B B B B B B B B B B\r\nLevels: A B\r\n\r\n\r\n\r\nboxplot(ozonoAB~ etiqueta, notch=T,\r\n        xlab=\"Invernadero\",ylab=\"Ozono\")\r\n\r\n\r\n\r\n\r\nSi usamos el ojimetro, parece ser que no sus medianas no son\r\ndistintas porque los intervalos intercuartil no se sobrelapan. Ahora\r\nhagamos una prueba de t (para comparar las medias) a pie.\r\n¬øQue necesitamos?\r\nlos grados de libertad. dijimos que para una prueba de dos\r\npoblaciones calculamos el total de observaciones (20) menos el num. de\r\npar√°metros estimados antes de realizar la prueba (dos medias). As√≠ que\r\ntenemos 18 g.l.\r\nNecesitamos calcualar las varianzas individuales de cada\r\ninvernadero, para poder calcular la diferencia de EE.\r\n\r\n\r\ns2A<-var(gardenA)\r\ns2B<-var(gardenB)\r\n\r\n\r\n\r\n¬øLuego que sigue?\r\nCalculamos la t de student para la diferencia de medias\r\n\r\n\r\n(mean(gardenA)-mean(gardenB))/sqrt(s2A/10+s2B/10)\r\n\r\n\r\n[1] -3.872983\r\n\r\nNoten que podemos ignorar el signo de la t. Porque solo depende que\r\ncual media pusimos primero. El valor absoluto es lo que importa.\r\nAhora necesitamos el valor cr√≠tico de la distribuci√≥n de t de\r\nreferencia para determinar si aceptamos o rechazamos la hip√≥tesis de que\r\nno hay diferenicas entre medias (que vienen de la misma poblaci√≥n). Se\r\ntrata de un problema de una o dos colas?\r\nComo no me interesa cual es mayor o menor, ni tengo ninguna hip√≥tesis\r\nde que invernadero deb√≠a tener mas o menos ozono, entonces es de dos\r\ncolas, por lo tanto uso una probabilidad de?\r\n\r\n\r\nqt(0.975,18)\r\n\r\n\r\n[1] 2.100922\r\n\r\n¬øAcepto o rechazo la hip√≥tesis de que son iguales?\r\nEn virtud de que el valor calculado es mayor que el valor cr√≠tico se\r\nrechaza la hip√≥tesis nula. (recuerden m√°s alto=rechazo Ho m√°s\r\nbajo=acepto)\r\nFinalmente necesito saber cual es la probabilidad de que encuentre yo\r\nla diferencia entre estas dos muestras (o una m√°s extrema) a pesar de\r\nque provienen de poblaciones con la misma media. Recuerden que es un\r\nproblema de dos colas y por esa raz√≥n necesito calcular pt y despu√©s\r\nmultiplicarlos por dos (para los dos extremos).\r\n\r\n\r\np<-2*pt(-3.872983,18)\r\np\r\n\r\n\r\n[1] 0.00111454\r\n\r\npara calcular los intervalos de confianza para la diferencia\r\nmedias\r\n\r\n\r\ndifmed<-mean(gardenA)-mean(gardenB)\r\nEEdifmed<-sqrt(s2A/10+s2B/10)\r\nt.de.tablas.alfa.05.gl.18<-qt(0.975,18)\r\nt.de.tablas.alfa.05.gl.18\r\n\r\n\r\n[1] 2.100922\r\n\r\n\r\n\r\nIC95<-(EEdifmed)*(t.de.tablas.alfa.05.gl.18)\r\nCotasup<- difmed+IC95\r\nCotasup\r\n\r\n\r\n[1] -0.9150885\r\n\r\n\r\n\r\nCotainf<- difmed-IC95 \r\nCotainf\r\n\r\n\r\n[1] -3.084911\r\n\r\n\r\n\r\nIC95t<-(sqrt(s2A/10+s2B/10))*(qt(0.975,18))\r\n  IC95t\r\n\r\n\r\n[1] 1.084911\r\n\r\n¬øCual es la probabilidad de que suceda? Ahora el autom√°tico\r\n\r\n\r\npruebat<-t.test(gardenA,gardenB)\r\npruebat\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  gardenA and gardenB\r\nt = -3.873, df = 18, p-value = 0.001115\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -3.0849115 -0.9150885\r\nsample estimates:\r\nmean of x mean of y \r\n        3         5 \r\n\r\nEntonces reporto la concentraci√≥n de ozono fue significativamente m√°s\r\nalta en el invernadero B (5.0 ppm) que en el A(3.0ppm;\r\nt=3.87, p=0.001(dos colas),\r\ngl=18)\r\nAhora, pudi√©ramos tener el problema de que las varianzas no son\r\niguales. entonces antes de hacer una prueba de t, necesitamos\r\npreguntarnos si las varianzas son significativamente distintas.\r\nComparemos las de los invernaderos B y C.\r\n\r\n\r\nvar(gardenB)\r\n\r\n\r\n[1] 1.333333\r\n\r\nvar(gardenC)\r\n\r\n\r\n[1] 14.22222\r\n\r\n¬øSe acuerdan que la distribuci√≥n de F era la adecuada para comparar\r\nvarianzas? la pregunta es, ¬øcual es la probabilidad de que estas dos\r\nmuestras hayan sido sacada de poblaciones que tienen la misma varianza?\r\nEl valor de F es simplemente el cociente de las varianzas\r\n\r\n\r\nCocienteF<-var(gardenC)/var(gardenB)\r\nCocienteF\r\n\r\n\r\n[1] 10.66667\r\n\r\nAhora comparo con la distribuci√≥n de probabilidad de F para los\r\ngrados de libertad correspondientes, y como no tengo ninguna idea de\r\ncual de las muestras debiera tener una varianza m√°s alta, entonces es\r\nuna prueba de dos colas.\r\nComo F no es sim√©trica, necesito calcular ambos lados.\r\n\r\n\r\nqf(0.975,9,9)\r\n\r\n\r\n[1] 4.025994\r\n\r\nqf(0.025,9,9)\r\n\r\n\r\n[1] 0.2483859\r\n\r\n¬øQue concluyo?\r\n¬øCual es la probabilidad de que estas dos muestras provengan de\r\npoblaciones con la misma varianza?\r\n\r\n\r\n2*(1-pf(CocienteF,9,9))\r\n\r\n\r\n[1] 0.001624199\r\n\r\nAhora el autom√°tico\r\n\r\n\r\nvar.test(gardenB,gardenC)\r\n\r\n\r\n\r\n    F test to compare two variances\r\n\r\ndata:  gardenB and gardenC\r\nF = 0.09375, num df = 9, denom df = 9, p-value = 0.001624\r\nalternative hypothesis: true ratio of variances is not equal to 1\r\n95 percent confidence interval:\r\n 0.02328617 0.37743695\r\nsample estimates:\r\nratio of variances \r\n           0.09375 \r\n\r\nAhora, si encontr√°ramos que suponer que las poblaciones de las que\r\nprovienen las muestras no se distribuyen de manera normal entonces\r\ntenemos la alternativa de la prueba de Wilcoxon de suma de rangos. Esta\r\nes id√©ntica en su sistema a la de los rangos signados. Veamos como\r\nLo primero que hago es poner todas las observaciones en un mismo\r\nvector\r\n\r\n\r\nozone<-c(gardenA,gardenB)\r\nozone\r\n\r\n\r\n [1] 3 4 4 3 2 3 1 3 5 2 5 5 6 7 4 4 3 5 6 5\r\n\r\nAhora les hago sus etiquetas para que no se me pierdan\r\n\r\n\r\netiqueta<-c(rep(\"A\",10),rep(\"B\",10))\r\netiqueta\r\n\r\n\r\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\r\n[17] \"B\" \"B\" \"B\" \"B\"\r\n\r\nAhora hago un vector de los rangos con la funci√≥n rank\r\n\r\n\r\nrangoscomb<-rank(ozone)\r\nrangoscomb\r\n\r\n\r\n [1]  6.0 10.5 10.5  6.0  2.5  6.0  1.0  6.0 15.0  2.5 15.0 15.0 18.5\r\n[14] 20.0 10.5 10.5  6.0 15.0 18.5 15.0\r\n\r\nNoten que para todos los valores repetidos se hace un promedio de los\r\nrangos que les tocan.\r\n\r\n\r\ntapply(rangoscomb,etiqueta,sum)\r\n\r\n\r\n  A   B \r\n 66 144 \r\n\r\nFinalmente uso el valor m√°s peque√±o (66) para compararlo con el valor\r\nde tablas para el etad√≠stico W para n de 10 y 10 y un alfa del 5%\r\n(W=78). Como nuestro valor es menor\r\n(porque uso como referencia el mas peque√±o del par que obtuve), entonces\r\nrechazo mi H0. Las Medias son significativamente distintas.\r\nAhora el autom√°tico.\r\n\r\n\r\nwilcox.test(gardenA,gardenB)\r\n\r\n\r\n\r\n    Wilcoxon rank sum test with continuity correction\r\n\r\ndata:  gardenA and gardenB\r\nW = 11, p-value = 0.002988\r\nalternative hypothesis: true location shift is not equal to 0\r\n\r\nLas diferencias se deben a que R utiliza un algoritmo de aproximaci√≥n\r\npara calcular valores de z. Es ligeramente distinto del tradicional que\r\nhicimos arriba. La mec√°nica es la misma. Noten la diferencia en las p de\r\nt y W. Wilcoxon es menos poderoso (95%). Pero es el correcto si las\r\ndistribuciones son sesgadas. Tambi√©n es correcto si no lo son (aunque\r\npoco menos poderoso). t no es correcta si hay desviaciones sustanciales\r\nde la normalidad.\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:35:32-06:00"
    },
    {
      "path": "Phdospobbis.html",
      "title": "Script Prueba de hip√≥tesis de dos poblaciones-bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.3\r\nC.3\r\nTarea\r\nAqu√≠ va la segunda parte de su tarea\r\n\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.3\r\nEntonces ¬øcual es la probabilidad de que mi muestra de frecuencias (o una mas extrema) pertenezca a una poblaci√≥n donde el aislamiento de la planta y el plumaje del colibr√≠ defensor son independientes?\r\n\r\n\r\n1-pchisq(35.34,1)\r\n\r\n\r\n[1] 2.768866e-09\r\n\r\n¬øCual es el valor cr√≠tico para rechazar la H0 con un alfa de 0.05?\r\n\r\n\r\nqchisq(0.95,1)\r\n\r\n\r\n[1] 3.841459\r\n\r\nAhora vamos a hacerlo de manera autom√°tica en R\r\n\r\n\r\ncolibries<-matrix(c(38,14,11,51),nrow=2)\r\ncolibries\r\n\r\n\r\n     [,1] [,2]\r\n[1,]   38   11\r\n[2,]   14   51\r\n\r\nchisq.test(colibries)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test with Yates' continuity correction\r\n\r\ndata:  colibries\r\nX-squared = 33.112, df = 1, p-value = 8.7e-09\r\n\r\nNoten que estos valores son un poco distintos porque se aplic√≥ una correcci√≥n de Yates. Esta fue dise√±ada para frecuencias peque√±as, pero ahora existen pruebas mejores para frecuencias peque√±as (20% o m√°s frecuencias menores a 5) como La Prueba exacta de Fisher Crawley p.¬†90 QyK p.388.\r\nEntonces le quito la correcci√≥n\r\n\r\n\r\nchisq.test(colibries,correct=F)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  colibries\r\nX-squared = 35.334, df = 1, p-value = 2.778e-09\r\n\r\nme da exactamente lo que calculamos a mano.\r\nC.3\r\nTarea\r\nAqu√≠ va la segunda parte de su tarea\r\nEl problema se trata de un investigador que esta interesado en si las hormigas construyen preferentemente sus nidos en √°rboles de una de dos especies. Entonces muestre√≥ 100 √°rboles de cada una de las especies. Encontr√≥ que 60 √°rboles de la especie A y 20 de la especie B ten√≠an nidos. ¬øEsta muestra apoya la hip√≥tesis de preferencia?\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:37-06:00"
    },
    {
      "path": "Phunapob.html",
      "title": "Script Prueba de hip√≥tesis de una poblacion",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nC.1\r\nR.2\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nVeamos un ejemplo de prueba de hip√≥tesis para una poblaci√≥n (Crawley\r\np.64). Estos son datos de Michelson (1978), de medidas tomadas para\r\nestimar la velocidad de la luz. Que ahora sabemos es cercano (299,990 km\r\np seg).\r\n\r\n\r\nveluz<-read.table(\"light.txt\",header=T)\r\nattach(veluz)\r\nnames(veluz)\r\n\r\n\r\n\r\n\r\n\r\nhist(speed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(speed)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n    650     850     940     909     980    1070 \r\n\r\nA todos los valores se les ha restado 299 000 para facilitar su\r\nvisualizaci√≥n ¬øque pueden ver aqu√≠? ¬øtenemos valores at√≠picos?\r\n\r\n\r\nboxplot(speed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nqqnorm(speed)\r\nqqline(speed,lty=2)\r\n\r\n\r\n\r\n\r\nLa muestra no se distribuye de acuerdo a lo esperado normalmente\r\n(cosa que una prueba de t para una poblaci√≥n asume), pero adem√°s ¬øCreen\r\nque la poblaci√≥n de referencia se distribuya de manera normal?\r\nNo puede porque tiene el problema de que la vel de la luz no puede\r\ntomar valores negativos\r\nNuestra hip√≥tesis es que los datos de Michelson difieren de el valor\r\nprevaleciente en esa √©poca como la vel de la luz 299,990 km/seg. Como a\r\ntodos los valores les quitaron 299 000 entonces ¬øcual va a ser la\r\nreferencia?\r\nAhora, si cumpli√©ramos con el supuesto de que la pob. de referencia\r\nse distribuye normalmente, c√≥mo resolvemos este problema?‚Ä¶ustedes ya lo\r\nsaben hacer.\r\nPrimero. ¬øEste problema tiene una o dos colas que le pisen? Entonces\r\n¬øque har√≠an?\r\nBueno, pero como no cumplimos con el primero de los supuestos,\r\nnecesitamos otra alternativa. Que se les ocurre?\r\nPor supuesto que tambi√©n podemos usar una t√©cnica de remuestreo con\r\nremplazo! Este es el punto 1. de la tarea de hoy. Hagan la\r\nprueba con bootstrap\r\nExiste otra alternativa es una prueba llamada de rangos signados de\r\nWilcoxon.\r\nC.1\r\nR.2\r\n\r\n\r\nlibrary(stats)\r\n\r\nwilcox.test(speed,mu=990)\r\n\r\n\r\n\r\n    Wilcoxon signed rank test with continuity correction\r\n\r\ndata:  speed\r\nV = 22.5, p-value = 0.00213\r\nalternative hypothesis: true location is not equal to 990\r\n\r\nLa probabilidad de obtener la media de nuestra muestra (estad√≠stico)\r\nen una poblaci√≥n de las medias de muchas muestras con media 990\r\n(par√°metro) es del 0.2%. Como aceptamos una probabilidad de equivocarnos\r\nal rechazar una H0 cuando esta es cierta del 5% entonces, la\r\nrechazamos!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:32:12-06:00"
    },
    {
      "path": "Phunapobbis.html",
      "title": "Script Prueba de hip√≥tesis de una poblacion-bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nC.2\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nC.2\r\nVamos a escribir una funci√≥n para calcular sesgo.\r\n\r\n\r\nskew<-function(x){\r\nm3<-sum((x-mean(x))^3)/length(x)\r\ns3<-sqrt(var(x))^3\r\nm3/s3  }\r\n\r\n\r\n\r\nveamos los datos skewdata\r\n\r\n\r\ndata<-read.table(\"skewdata.txt\",header=T)\r\nattach(data)\r\nnames(data)\r\n\r\n\r\n\r\n\r\n\r\nhist(values)\r\n\r\n\r\n\r\n\r\nCalculemos el sesgo\r\n\r\n\r\nskew(values)\r\n\r\n\r\n[1] 1.318905\r\n\r\nAhora hagamos una prueba de t para determinar si hay sesgo respecto a\r\nlo esperado para una poblaci√≥n de inferencia con distribuci√≥n normal\r\n\r\n\r\nskew(values)/sqrt(6/length(values))\r\n\r\n\r\n[1] 2.949161\r\n\r\n1-pt(2.949,28)\r\n\r\n\r\n[1] 0.003185136\r\n\r\n¬øCual es la conclusi√≥n?. Efectivamente est√° m√°s sesgado de los\r\nesperado normalmente, ¬øque se puede hacer, si insistimos en cumplir con\r\nla normalidad?\r\n\r\n\r\nskew(sqrt(values))/sqrt(6/length(values))\r\n\r\n\r\n[1] 1.474851\r\n\r\nskew(log(values))/sqrt(6/length(values))\r\n\r\n\r\n[1] -0.6600605\r\n\r\n\r\n\r\nkurtosis<-function(x) {\r\nm4<-sum((x-mean(x))^4)/length(x)\r\ns4<-var(x)^2\r\nm4/s4 - 3  }\r\nkurtosis(values)\r\n\r\n\r\n[1] 1.297751\r\n\r\nkurtosis(values)/sqrt(24/length(values))\r\n\r\n\r\n[1] 1.45093\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:32:23-06:00"
    },
    {
      "path": "Regresionsimp.html",
      "title": "Script Modelos lineales",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nC.1\r\nR.2\r\nC.2\r\nR.3\r\nC.3\r\nR.4\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nLlamo los datos, los coloco en un dataframe y convierto las columnas\r\nen variables\r\n\r\n\r\nreg.data<-read.table(\"tannin.txt\",header=T)\r\nattach(reg.data)\r\nnames(reg.data)\r\n\r\n\r\n\r\nGr√°fico\r\n\r\n\r\npar(mfrow=c(1,1))\r\nplot(tannin,growth,pch=16)\r\n\r\n\r\n\r\n\r\n¬øLa tendencia de la variable de respuesta es a incrementar o a\r\ndisminuir con la explicatoria? tendencia a disminuir\r\n¬øEs factible que los datos sean explicados por una l√≠nea horizontal?\r\nH0.\r\n\r\n\r\nplot(tannin,growth,pch=16)\r\nabline(mean(growth),0)\r\n\r\n\r\n\r\n\r\nLa H0 no parece factible, entonces b es probablemente dif de 0 y\r\nnegativa\r\n¬øSi existe una tendencia es recta o curva? relaci√≥n recta, entonces\r\nproponemos el modelo \\(y=a+bx+e\\)\r\n¬øLa dispersi√≥n de los datos es uniforme a lo largo de la l√≠nea o\r\ncambia sistem√°ticamente con la variable explicatoria?. Dispersi√≥n muy\r\nuniforme, la ordenada al origen es dif de 0 entonces a es prob mayor que\r\n0.\r\n¬øA ojo cuales son los valores de a y b? Como podemos hacer este\r\nproceso sistem√°tico y preciso?\r\nC.1\r\nR.2\r\nY la variaci√≥n total de y es la dispersi√≥n de los datos alrededor de\r\ny barra.\r\nLa Suma de Cuadrados Total es \\(SCT=\r\n\\sum(y-\\overline{y})^2\\)\r\n\r\n\r\nplot(tannin,growth,pch=16)\r\nabline(mean(growth),0)\r\nfor (i in 1:9) lines(c(tannin[i],tannin[i]),c(growth[i],mean(growth)))\r\n\r\n\r\n\r\n\r\nLa mejor recta ajustada por el m√©todo de m√≠nimos cuadrados es aquella\r\nque minimiza la Suma de Cuadrados de las desviaciones de los valores de\r\ny de la l√≠nea ajustada \\(\\hat{y}\\),\r\n\\(SCE=\\sum(y - \\hat{y})^2\\)\r\n\r\n\r\nplot(tannin,growth,pch=16) \r\nabline(lm(growth~tannin))\r\n\r\n\r\n\r\n\r\n\r\n\r\nysomb <- predict(lm(growth ~ tannin))\r\nplot(tannin,growth,pch=16) \r\nabline(lm(growth~tannin))\r\nfor(i in 1:9) lines(c(tannin[i], tannin[i]), c(growth[i], ysomb[i]))\r\n\r\n\r\n\r\n\r\nC.2\r\nR.3\r\nAhora bien, una tercera cantidad es la Suma de Cuadrados de la\r\nRegresi√≥n (es decir del efecto de la variable predictora)\r\n\\[SCR = SCTotal - SCError\\]\r\n\r\n\r\nplot(tannin, growth, type = \"n\") \r\nabline(mean(growth), 0) \r\nmodelito <- lm(growth ~ tannin) \r\nabline(modelito) \r\nfor(i in 1:9) lines(c(tannin[i], tannin[i]), c(mean(growth), predict(modelito)[i])) \r\npoints(tannin,predict(modelito), pch = 16) \r\npoints(tannin, growth)\r\n\r\n\r\n\r\n\r\nC.3\r\nR.4\r\nEmpezamos a ajustar los modelos: modelo nulo - solo la media\r\n\r\n\r\nNulo <- lm(growth ~ 1) \r\nnames(Nulo)\r\n\r\n\r\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \r\n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \r\n [9] \"call\"          \"terms\"         \"model\"        \r\n\r\n\r\n\r\nanova(Nulo) \r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: growth\r\n          Df Sum Sq Mean Sq F value Pr(>F)\r\nResiduals  8 108.89  13.611               \r\n\r\n\r\n\r\nNulo$df.residual \r\n\r\n\r\n[1] 8\r\n\r\nNulo$coefficients \r\n\r\n\r\n(Intercept) \r\n   6.888889 \r\n\r\nNulo$fitted.values\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n6.888889 6.888889 6.888889 6.888889 6.888889 6.888889 6.888889 \r\n       8        9 \r\n6.888889 6.888889 \r\n\r\n\r\n\r\nplot(tannin, growth) \r\nabline(a=Nulo$coe, b=0) \r\nabline(Nulo$coe, 0)\r\n\r\n\r\n\r\n\r\nEs decir solo se ha ajustado la media que no ofrece informaci√≥n\r\nimportante\r\nAgregamos el efecto del tannin\r\n\r\n\r\nTanino <- update(Nulo, . ~ . + tannin)\r\nTanino\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nCoefficients:\r\n(Intercept)       tannin  \r\n     11.756       -1.217  \r\n\r\n\r\n\r\nanova(Tanino)\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: growth\r\n          Df Sum Sq Mean Sq F value    Pr(>F)    \r\ntannin     1 88.817  88.817  30.974 0.0008461 ***\r\nResiduals  7 20.072   2.867                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nTanino$coefficients\r\n\r\n\r\n(Intercept)      tannin \r\n  11.755556   -1.216667 \r\n\r\n\r\n\r\nsummary(Tanino)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4556 -0.8889 -0.2389  0.9778  2.8944 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.7556     1.0408  11.295 9.54e-06 ***\r\ntannin       -1.2167     0.2186  -5.565 0.000846 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.693 on 7 degrees of freedom\r\nMultiple R-squared:  0.8157,    Adjusted R-squared:  0.7893 \r\nF-statistic: 30.97 on 1 and 7 DF,  p-value: 0.0008461\r\n\r\nO bien pedimos la secuencia de ajustes, que produce estos cambios en\r\ndevianza\r\n\r\n\r\nanova(Nulo, Tanino)\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: growth ~ 1\r\nModel 2: growth ~ tannin\r\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \r\n1      8 108.889                                  \r\n2      7  20.072  1    88.817 30.974 0.0008461 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nsummary(Tanino)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4556 -0.8889 -0.2389  0.9778  2.8944 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.7556     1.0408  11.295 9.54e-06 ***\r\ntannin       -1.2167     0.2186  -5.565 0.000846 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.693 on 7 degrees of freedom\r\nMultiple R-squared:  0.8157,    Adjusted R-squared:  0.7893 \r\nF-statistic: 30.97 on 1 and 7 DF,  p-value: 0.0008461\r\n\r\nSi queremos, podemos guardar los valores ajustados y los residuales\r\nen la base de datos:\r\n\r\n\r\nreg.data$ajustados <- fitted.values(Tanino) \r\nreg.data$residuales <- residuals(Tanino)\r\nreg.data\r\n\r\n\r\n  growth tannin ajustados residuales\r\n1     12      0 11.755556  0.2444444\r\n2     10      1 10.538889 -0.5388889\r\n3      8      2  9.322222 -1.3222222\r\n4     11      3  8.105556  2.8944444\r\n5      6      4  6.888889 -0.8888889\r\n6      7      5  5.672222  1.3277778\r\n7      2      6  4.455556 -2.4555556\r\n8      3      7  3.238889 -0.2388889\r\n9      3      8  2.022222  0.9777778\r\n\r\nPara inspeccionar qu√© tan bueno es el modelo existen algunos recursos\r\ngr√°ficos donde se examinan la distribuci√≥n de los residuales y los\r\npuntos extremos que que pueden ‚Äúcargar‚Äù el valor num√©rico de los\r\npar√°metros:\r\n\r\n\r\npar(mfcol=c(2,2))\r\nplot(Tanino)\r\n\r\n\r\n\r\n\r\nExaminamos un modelo sin el dato extremo:\r\n\r\n\r\nSindat7 <- lm(growth[-7] ~ tannin[-7]) \r\nsummary(Sindat7)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth[-7] ~ tannin[-7])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.4549 -0.9572 -0.1622  0.4572  2.6622 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.6892     0.8963  13.042 1.25e-05 ***\r\ntannin[-7]   -1.1171     0.1956  -5.712  0.00125 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.457 on 6 degrees of freedom\r\nMultiple R-squared:  0.8446,    Adjusted R-squared:  0.8188 \r\nF-statistic: 32.62 on 1 and 6 DF,  p-value: 0.001247\r\n\r\nNo ganamos gran cosa\r\nPara predecir valores usamos:\r\n\r\n\r\npredict(Tanino, list(tannin =7.5))\r\n\r\n\r\n       1 \r\n2.630556 \r\n\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nls() \r\n\r\n\r\n[1] \"i\"        \"modelito\" \"Nulo\"     \"reg.data\" \"Sindat7\"  \"Tanino\"  \r\n[7] \"ysomb\"   \r\n\r\nrm(list=ls(all=TRUE))\r\n\r\n\r\n\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:37:25-06:00"
    },
    {
      "path": "Tarea_1.html",
      "title": "Tarea 1",
      "author": [],
      "contents": "\r\n\r\nContents\r\nTeor√©ma de tendencia\r\ncentral\r\n\r\nTeor√©ma de tendencia central\r\nSupongamos que tenemos una muestra de 100 colibr√≠es de una especie A.\r\nLes hemos medido la extensi√≥n de alas y encontramos que la media es 17\r\ncm y la desviaci√≥n est√°ndar es de 0.8 cm.\r\nDibujen la distribuci√≥n de probabilidad bajo un supuesto de\r\nnormalidad\r\n¬øque probabilidad hay de encontrar un caso con una extensi√≥n\r\nmayor a 18cm?\r\nNoten que cualquier valor de y en una distribuci√≥n normal se puede\r\nconvertir en un valor de z. Recuerden que \\(z=(y-media(y))/desv.stand\\).\r\n¬øy la de encontrar un ave con una extensi√≥n menor a 15?\r\n¬øustedes creen que un ave con una extensi√≥n de 15cm puede decirse\r\nque con una confianza del 95% pertenece a la misma poblaci√≥n?\r\n¬øentre que medidas de extensi√≥n de alas se puede decir con un 95%\r\nde confianza que las aves pertenecen a esa poblaci√≥n?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T11:44:56-06:00"
    },
    {
      "path": "Tarea_2.html",
      "title": "Tarea 2",
      "author": [],
      "contents": "\r\n\r\nContents\r\nEstimaci√≥n\r\nPreguntas:\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nEstimaci√≥n\r\nLa tabla de datos de Palmer.xlsx contiene los Valores de\r\nImportancia (VI) de 25 especies de briofitas (musgos y l√≠quenes)\r\ncreciendo en troncos de √°rboles en tres sitios en el Duke Forest, en\r\nCarolina del Norte. Los VIs son una medida de abundancia relativa. Cada\r\nvalor (por celda) es el promedio de 10 √°rboles por sitio.\r\nLos c√≥digos de las muestras indican las especies de √°rbol:\r\nBN = Betula nigra, abedul;\r\nLT = Liriodendron tulipifera, √°rbol de tulip√°n;\r\nPE = Pinus echinata, pino de hoja chica;\r\nPO = Platanus occidentalis, ficus;\r\nPT = Pinus taeda, pino hoja mediana;\r\nQR= Quercus rubra, encino rojo;\r\nQA= Quercus alba, encino blanco.\r\nPreguntas:\r\n¬øCu√°l crees que sea la estimaci√≥n m√°s eficiente de la tendencia\r\ncentral para Isopterygium tenerum?\r\nEstima con un 93% de confianza en que intervalo se encuentra la\r\nmedia para 4 especies de briofitos (utilicen un m√©todo basado en\r\ndistribuciones y uno en remuestreo).\r\n¬øQu√© puedes decir para las distintas especies? ¬ødifieren las\r\nestimaciones dependiendo del m√©todo? ¬øPorqu√©?\r\n¬øC√≥mo reportar√≠as la media general para Platygyrium repens en un\r\nart√≠culo cient√≠fico?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:13:18-06:00"
    },
    {
      "path": "Tarea_3.html",
      "title": "Tarea 3",
      "author": [],
      "contents": "\r\n\r\nContents\r\nRemuestreo\r\nen pruebas de hip√≥tesis con una muestra simple\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nRemuestreo\r\nen pruebas de hip√≥tesis con una muestra simple\r\nEstos son datos de Michelson (1978) de medidas tomadas para estimar\r\nla velocidad de la luz que ahora sabemos es (299,792.458 km p seg).\r\nUse el cuerpo de datos contenidos en light.txt y\r\nutilizando el m√©todo de remuestreo calcule los intervalos de confianza\r\n(95%) de la media de la velocidad (speed). ¬øEl valor medio est√° dentro\r\ndel intervalo de confianza del muestreo?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:18:02-06:00"
    },
    {
      "path": "Tarea_4.html",
      "title": "Tarea 4",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPrueba\r\nde hip√≥tesis\r\n\r\nPrueba de hip√≥tesis\r\nUn investigador est√° interesado en probar si las hormigas construyen\r\npreferentemente sus nidos en √°rboles de una de dos especies. Muestre√≥\r\n100 √°rboles de cada una de las especies y encontr√≥ que 60 √°rboles de la\r\nespecie A y 20 de la especie B ten√≠an nidos. ¬øEsta muestra apoya la\r\nhip√≥tesis de preferencia?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T11:23:21-06:00"
    },
    {
      "path": "Tarea_5.html",
      "title": "Tarea 5",
      "author": [],
      "contents": "\r\n\r\nContents\r\nRegresi√≥n\r\nsimple\r\nRecuerda\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nRegresi√≥n simple\r\nLos datos de masas esquel√©ticas y corporales (todas en kg) del\r\narchivo pesoaves.txt fueron tomados al azar de un estudio\r\nm√°s amplio realizado por Prange et al.¬†(1979, American Naturalist 113:\r\n103-122) donde analizaban la hip√≥tesis de que las aves tienen esqueletos\r\nm√°s ligeros que los mam√≠feros debido a su adaptaci√≥n al vuelo. Las dos\r\nprimeras columnas correponden a masas de esqueleto y de cuerpo de aves\r\nrespectivamente. Las dos √∫ltimas columnas a masas esquel√©tica y\r\ncorporal, respectivamente, de mam√≠feros.\r\nEncuentra un modelo m√≠nimo adecuado que represente la relaci√≥n\r\nalom√©trica entre masa esquel√©tica y masa corporal para aves por un lado\r\ny para mam√≠feros por el otro. ¬øC√≥mo reportar√≠a los resultados obtenidos\r\nen un art√≠culo cient√≠fico?\r\nEstima con dichos modelos cu√°l ser√≠a la masa esquel√©tica esperada\r\npara un ave de 5 kg de masa corporal y cu√°l ser√≠a la masa esquel√©tica de\r\nun mam√≠fero de 5 kg.\r\nEn un an√°lisis cualitativo (por ejemplo sobreponiendo las curvas\r\npredichas) crees que se sostiene la hip√≥tesis de los autores?\r\nRecuerda\r\n‚Ä¢ Realizar el an√°lisis exploratorios (gr√°ficos y tablas)\r\npertinentes.\r\nEjecutar los an√°lisis en R, por favor incluye √∫nicamente las\r\nsalidas del programa que son ESTRICTAMENTE NECESARIAS\r\npara respaldar tus decisiones -\r\nEs IN√öTIL incluir en el reporte salidas del\r\nprograma SIN INTERPRETACI√ìN.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:17:49-06:00"
    },
    {
      "path": "try.html",
      "title": "Videos Semana 2",
      "author": [],
      "contents": "\r\nhttps://drive.google.com/file/d/1J6ELC-oqQrsTNsTsSGfVGyV8iBELnJGG/view?usp=sharing\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-17T15:49:11-06:00"
    }
  ],
  "collections": []
}
