{
  "articles": [
    {
      "path": "ANDEVA.html",
      "title": "Script ANDEVA de una via",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nConceptos básicos\r\ndel análisis de la varianza\r\n\r\nC.1\r\nR.2\r\nR.3\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nConceptos básicos\r\ndel análisis de la varianza\r\nLa hipótesis nula en un análisis de la varianza tipo I común es:\r\n\\[H0: m1 = m2 = m3 = ... = mk\\]\r\n¿Cómo es que esta hipótesis se pone a prueba en un ANDEVA? Por cierto\r\nesta es una prueba “omnibus”, es decir ¡prueba muchas cosas de un\r\njalón!\r\nPara ver como es que opera el anova veamos el ejemplo que sigue:\r\nTomemos un solo factor, “f”, con dos niveles y pongamos los datos en una\r\ngráfica simple, según el orden en el que fueron obtenidas las\r\nmediciones.\r\n\r\n\r\nanova<-read.table(\"anova.data.txt\",header=T)\r\nattach(anova)\r\nnames(anova)\r\n\r\n\r\n\r\n\r\n\r\nplot(y)\r\nabline(mean(y), 0, col=4)\r\nfor (i in 1:length(y)) lines (c(i,i), c(mean(y), y[i]), col=5)\r\n\r\n\r\n\r\n\r\n¿Qué muestra esta gráfica? ¿A que equivale la suma de los trazos\r\nverticales?\r\n\\[ SCT=\\Sigma(y-\r\n\\overline{y})^2\\]\r\nAhora vamos a hacer exactamente lo mismo, pero ahora dividiendo por\r\ncada uno de los niveles del factor F. Incorporemos la información del\r\nfactor “f”. Para esto hay que calcular los promedios de “y” que\r\ncorresponden a los niveles de “f”\r\n\r\n\r\npromedios <- tapply(y, f, mean)\r\n\r\n\r\n\r\nGrafiquemos esta nueva estructura de datos sobre la gráfica que ya\r\ntenemos\r\n\r\n\r\nplot(y)\r\nlines(c(1, 7), c(promedios[1], promedios[1]), col = 2)\r\nlines(c(7, 14), c(promedios[2], promedios[2]), col = 5)\r\nfor (i in 1:7 ) lines (c(i,i), c(promedios[1], y[i]), col = 1, lty=6)\r\nfor (i in 8:14) lines (c(i,i), c(promedios[2], y[i]), col = 1, lty=6)\r\n\r\n\r\n\r\n\r\n¿Qué muestra esta gráfica? ¿a que equivale la suma de los trazos\r\npunteados verticales?\r\n\\[SCE= \\Sigma(y_1-\\hat{y_1})^2 +\r\n\\Sigma(y_2- \\hat{y_2})^2\\]\r\nSi las dos medias fueran iguales ¿cómo compararían estas dos\r\ngráficas?\r\nSi lo piensan tendrían que ser iguales porque las medias de los\r\nniveles del tratamiento se nivelarían a la misma altura. Si las medias\r\nson significativamente distintas ¿cual varianza sería mayor? la\r\ncalculada con SCT o la calculada con SCE? Esta es la razón por la cual\r\nel ANDEVA compara medias a través de la comparación de varianzas!!!!\r\n¿Qué interpretación tiene la diferencia entre las dos sumas\r\nmencionadas arriba? Pues es precisamente la varianza explicada por el\r\nmodelo. Esta diferencia se asocia con la siguiente gráfica:\r\n\r\n\r\nmodelo <- lm(y~f)\r\nplot (y)\r\nabline (mean(y), 0, col = 4)\r\npoints(predict(modelo), pch = 16, col = 5)\r\nfor (i in 1:14) lines(c(i, i), c(mean(y), predict(modelo)[i]), col = 6)\r\n\r\n\r\n\r\n\r\nC.1\r\nR.2\r\n¿Que implica el ajuste del modelo ANOVA del factor “f”?\r\n\r\n\r\nSCT<-sum((y-mean(y))^2)\r\nSCT\r\n\r\n\r\n[1] 55.5\r\n\r\nLa pregunta es cuanto de esta variación es explicada por diferencias\r\nentre las medias de A y B (niveles del factor F) y cuanto por el\r\nerror\r\n\r\n\r\nSCEa<-sum((y[f==\"a\"]-mean(y[f==\"a\"]))^2)\r\nSCEb<-sum((y[f==\"b\"]-mean(y[f==\"b\"]))^2)\r\n\r\n\r\n\r\nEntonces la SCE es la suma de estas dos cantidades\r\n\r\n\r\nSCE<-SCEa+SCEb\r\nSCE\r\n\r\n\r\n[1] 24\r\n\r\nFinalmente la SCA es SCT-SCE\r\n\r\n\r\nSCA<-SCT-SCE\r\nSCA\r\n\r\n\r\n[1] 31.5\r\n\r\nEntonces ya podemos llenar la tabla de ANOVA\r\n#C.2\r\nR.3\r\nAhora calculemos la F\r\n\r\n\r\n31.5/2\r\n\r\n\r\n[1] 15.75\r\n\r\ny la p\r\n\r\n\r\n1-pf(15.75,1,12)\r\n\r\n\r\n[1] 0.001864103\r\n\r\nAhora el automatico\r\n\r\n\r\nmodelo<-aov(y~f)\r\n\r\n\r\n\r\n\r\n\r\nsummary(modelo)\r\n\r\n\r\n            Df Sum Sq Mean Sq F value  Pr(>F)   \r\nf            1   31.5    31.5   15.75 0.00186 **\r\nResiduals   12   24.0     2.0                   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nboxplot(y~f,xlab=\"factor F\",ylab=\"y\")\r\n\r\n\r\n\r\n\r\n¿Cual es la conclusión? Ahora hacemos la crítica del modelo\r\n\r\n\r\npar(mfrow=c(2,2))\r\nplot(modelo)\r\n\r\n\r\n\r\n\r\ny ahora lo ultimo\r\n\r\n\r\nA<-c(6,8,5,9,7,8,6)\r\nB<-c(9,11,8,12,10,11,9)\r\nt.test(A,B)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  A and B\r\nt = -3.9686, df = 12, p-value = 0.001864\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -4.647028 -1.352972\r\nsample estimates:\r\nmean of x mean of y \r\n        7        10 \r\n\r\nsummary(modelo)\r\n\r\n\r\n            Df Sum Sq Mean Sq F value  Pr(>F)   \r\nf            1   31.5    31.5   15.75 0.00186 **\r\nResiduals   12   24.0     2.0                   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nLa anova es una “Generalización” de t para poder comparar mas de dos\r\nmedias. En realidad F=t^2\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:40:55-06:00"
    },
    {
      "path": "Correlacion.html",
      "title": "Script Correlación",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nVamos a ver una serie de datos para ver si existe una relación lineal\r\nentre ellos.\r\n\r\n\r\ndata<-read.table(\"twosample.txt\",header=T)\r\nattach(data)\r\ndata\r\n\r\n\r\n\r\n\r\n\r\nplot(x,y)\r\n\r\n\r\n\r\n\r\nSe acuerdan que necesitamos primero para calcular el coeficiente de\r\ncorrelación de pearson? Las varianzas individuales\r\n\r\n\r\nvar(x)\r\n\r\n\r\n[1] 199.9837\r\n\r\nvar(y)\r\n\r\n\r\n[1] 977.0153\r\n\r\n¿y que más? La covarianza y estamos hechos\r\n\r\n\r\nvar(x,y)\r\n\r\n\r\n[1] 414.9603\r\n\r\nAhora calculamos r\r\n\r\n\r\nvar(x,y)/sqrt(var(x)*var(y))\r\n\r\n\r\n[1] 0.9387684\r\n\r\nAhora hagamoslo en automático\r\n\r\n\r\ncor(x,y)\r\n\r\n\r\n[1] 0.9387684\r\n\r\nY ahora hagamos la prueba de hipótesis\r\nCalculamos EE de r\r\n\r\n\r\nEEr<-((1-(cor(x,y)^2))/(length(x)-2))^0.5\r\nEEr\r\n\r\n\r\n[1] 0.05025759\r\n\r\nCalculo t de la muestra\r\n\r\n\r\nte<-cor(x,y)/EEr\r\nte\r\n\r\n\r\n[1] 18.67914\r\n\r\nCalculo t de tablas\r\n\r\n\r\nqt(0.975,47)\r\n\r\n\r\n[1] 2.011741\r\n\r\nCalculo la p\r\n\r\n\r\n2*(1-pt(18.67914,47))\r\n\r\n\r\n[1] 0\r\n\r\nAhora hagamoslo de manera automática\r\n\r\n\r\npearson<-cor.test(x,y)\r\npearson\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  x and y\r\nt = 18.679, df = 47, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.8934139 0.9651786\r\nsample estimates:\r\n      cor \r\n0.9387684 \r\n\r\n¿Que nos falta?. Pues no sabemos si cumplimos con los supuestos.\r\nVeamos el de normalidad\r\n\r\n\r\npar(mfrow=c(1,2))\r\nqqnorm(x, main=\"Q-Q plot x\"); qqline(x, col = 2, lty = 2)\r\nqqnorm(y, main=\"Q-Q plot y\"); qqline(y, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\n¿Que opciones tengo?.\r\nHacer una prueba de sesgo y kurtosis para ver si estas\r\ndesviaciones son significativas\r\nSi son significativas, puedo intentar transformaciones o puedo\r\nutilizar muchas de las otras pruebas de correlación que son robustas a\r\nla violación de este supuesto. Vean Q y k p.76 y Crawley\r\np.97-102.\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:39:05-06:00"
    },
    {
      "path": "Estimacion1.html",
      "title": "Script Estimación 1 - El papel de la probabilidad",
      "author": [],
      "contents": "\r\n\r\nContents\r\nEl teorema de tendencia\r\ncentral\r\nR.1\r\nC.1\r\n\r\nR.2\r\nC.2\r\n\r\nR.3\r\nTarea de hoy\r\n¡Fin!\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nEl teorema de tendencia\r\ncentral\r\nR.1\r\nPidámosle a R que tome 10000 números al azar (0-10) y que grafique\r\ncon que frecuencia eligió cada número ¿que esperan ver?\r\n\r\n\r\nnumeros<-(runif(10000)*10)\r\n\r\n\r\n\r\n\r\n\r\nhist(runif(10000)*10,main=\"\")\r\n\r\n\r\n\r\n\r\nTomemos 5 números al azar del 0 al 10 y saquemos el promedio de los\r\ncinco números ¿cual creen que sea el promedio típico? Hagámonos\r\n\r\n\r\nclase<-c((mean(,,,,)), (mean(,,,,)))\r\nclase\r\n\r\n\r\n\r\nAhora pidámosle a R que lo haga 10000 veces (¡10000 muestras de\r\nnúmeros del 0 al 10!)\r\n\r\n\r\nmeans<-numeric(10000)\r\nfor (i in 0:10000){ \r\n  means[i]<- mean(runif(5)*10)}\r\n  \r\nhist(means,ylim=c(0,1600))\r\n\r\n\r\n\r\n\r\nNoten que existe una tendencia central. Lo que significa que el mero\r\nhecho de muestrear genera una tendencia central en el valor\r\npromedio.\r\nC.1\r\nR.2\r\nEl histograma de los números se ve como una distribución normal.\r\n¿porque no dibujamos una función normal sobre este histograma para ver\r\nque tan normal es? ¿Que necesitamos saber?\r\n\r\n\r\nmean(means)\r\n\r\n\r\n[1] 4.992107\r\n\r\nsd(means)\r\n\r\n\r\n[1] 1.3046\r\n\r\ny meterlo en la ecuación \\[ f(y) =\r\n\\frac{1/}{(\\sqrt{2 \\pi} \\sigma)} e^{y- \\mu / 2 \\sigma^2} \\] para\r\ncada valor de means verdad? R lo hace por ustedes\r\nPrimero generamos una serie de números para el eje x que le permita a\r\nR saber cada cuando poner un puntito de la línea. Si queremos una linea\r\nsuave una buena regla es 100 números. Como queremos números entre 0 y\r\n10:\r\n\r\n\r\nxv<-seq(0,10,0.1)\r\n\r\n\r\n\r\nComo la función normal tiene un área bajo la curva de 1 y nosotros\r\ntenemos 10000 valores, necesitamos escalar la curva multiplicándola por\r\nel número de valores que se encuentran de cada lado de el valor de en\r\nmedio (mediana).\r\n\r\n\r\nyv<-dnorm(xv,mean=4.993275,sd=1.291482)*5000\r\n\r\n\r\n\r\n\r\n\r\nhist(means,ylim=c(0,1600))\r\nlines(xv,yv)\r\n\r\n\r\n\r\n\r\nEl ajuste es perfecto! Lo interesante del teorema de tendencia\r\ncentral es que no importa cual sea la distribución real, si se toma una\r\nmuestra de ella, se comportará normalmente.\r\nC.2\r\nR.3\r\nEl estandarizar la curva normal tiene muchas ventajas. Nos permite\r\npor ejemplo saber cual es el área hasta cualquier valor del eje X\r\n(llamados desviaciones estándar).\r\nPidámosle a R que nos dibuje una Distribución Normal Estandarizada.\r\nRecuerden que lo primero que hay que hacer cuando se dibujan líneas\r\n\r\n\r\nnd<-seq(-3,3,0.01)\r\n\r\n\r\n\r\nCuando no le damos la media y a la var, significa que\r\nqueremos la estandarizada (el default), solo le dimos las\r\nx.\r\n\r\n\r\ny<-dnorm(nd)\r\nplot(nd,y,type=\"l\")\r\n\r\n\r\n\r\n\r\nAhora utilicemos pnorm para determinar que proporción de\r\nvalores caen por debajo de dos desviaciones estándar.\r\n\r\n\r\npnorm(-2)\r\n\r\n\r\n[1] 0.02275013\r\n\r\nque proporción? y ahora por debajo de 1 ds\r\n\r\n\r\npnorm(-1)\r\n\r\n\r\n[1] 0.1586553\r\n\r\n¿Que pasa si queremos saber que proporción de muestras caen A\r\nLA DERECHA de la desviación 3?\r\nesta es una opción\r\n\r\n\r\n 1-pnorm(3)\r\n\r\n\r\n[1] 0.001349898\r\n\r\n¿y la otra?\r\n\r\n\r\npnorm(-3)\r\n\r\n\r\n[1] 0.001349898\r\n\r\nEs claro que un valor de 3 cuando la media es 0 en una dn es muy poco\r\nprobable (0.13% de las veces cuando se saca una muestra de la pob.)\r\nAhora veamos un uso muy común de la distribución de z. Este es\r\npreguntarse bajo una situación aleatoria (solo por el hecho de muestrear\r\nuna población) entre que desviaciones estándar podemos encontrar el 95%\r\nde las muestras. (noten que al ser simétrica, tenemos que alojar 2.5% de\r\nlos casos “excedentes” de cada lado y para ello definimos un vector\r\ndentro de la funció qnorm (cuantiles normales)\r\n\r\n\r\nqnorm(c(0.025,0.975))\r\n\r\n\r\n[1] -1.959964  1.959964\r\n\r\nahora dibujémoslo\r\n\r\n\r\ny<-dnorm(nd)\r\n\r\nplot(nd,y,type=\"l\")\r\n  abline(v=-1.96)\r\n  abline(v=+1.96)\r\n\r\n\r\n\r\n\r\nEsto es muy importante porque si encontramos que nuestro muestreo no\r\nse ajusta a estos valores esperados entonces posiblemente pertenezcan a\r\ndos poblaciones distintas. Este es el principio básico de todas las\r\npruebas de significancia (p=0.05), el origen de el error estándar (1.96\r\ndesv. estándar), los límites de confianza etc.\r\nTarea de hoy\r\nSupongamos que tenemos una muestra de 100 colibríes de una especie A.\r\nLes hemos medido la extensión de alas y encontramos que la media es 17\r\ncm y la desviación estándar es de 0.8 cm.\r\nDibujen la distribución de probabilidad bajo un supuesto de\r\nnormalidad\r\n¿que probabilidad hay de encontrar un caso con una extensión\r\nmayor a 18cm?\r\nNoten que cualquier valor de y en una distribución normal se puede\r\nconvertir en un valor de z. Recuerden que \\(z=(y-media(y))/desv.stand\\).\r\n¿y la de encontrar un ave con una extensión menor a 15?\r\n¿ustedes creen que un ave con una extensión de 15cm puede decirse\r\nque con una confianza del 95% pertenece a la misma población?\r\n¿entre que medidas de extensión de alas se puede decir con un 95%\r\nde confianza que las aves pertenecen a esa población?\r\n¡Fin!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:14:25-06:00"
    },
    {
      "path": "Estimacion2.html",
      "title": "Script Estimación 2",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nEstimación de\r\nparámetros de tendencia central\r\n\r\nC.1\r\nR.2\r\nC.2\r\nR.3\r\nMedidas de dispersión\r\nC.3\r\nR.4\r\nC.4\r\nR.5\r\nC.5\r\nR.6\r\nC.6\r\nFin ———————————————————————\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar yvalues\r\n\r\n\r\nR.1\r\nEstimación de\r\nparámetros de tendencia central\r\nSabemos que aunque los datos no se agrupen alrededor de un valor\r\ntípico si hacemos muestras consecutivas los estadísticos de estas van a\r\ntener una tendencia central.\r\nVeamos un cuerpo de datos. Una variable y.\r\n\r\n\r\nyvals <- read.table(\"yvalues.txt\",header=T)\r\nattach(yvals)\r\nyvals\r\n\r\n\r\n\r\nUna manera muy simple de medir la tendencia central es ver cual es el\r\nvalor más frecuente. Este se denomina MODA\r\n\r\n\r\nyord<-sort(y)\r\nyord\r\n\r\n\r\n [1] 1.029680 1.032829 1.037292 1.053150 1.053251 1.063707 1.064455\r\n [8] 1.070355 1.071217 1.075693 1.081053 1.084013 1.086288 1.094628\r\n[15] 1.095193 1.100543 1.101244 1.102617 1.104005 1.108847 1.108853\r\n[22] 1.109046 1.110171 1.112230 1.117421 1.120611 1.121111 1.126974\r\n[29] 1.127780 1.128891 1.130020 1.130521 1.134060 1.145682 1.149131\r\n[36] 1.150670 1.154751 1.156906 1.190229\r\n\r\n\r\n\r\n#windows()\r\nhist(y)\r\n\r\n\r\n\r\n\r\n¿cual es la clase modal aquí?\r\nPero ahora queremos saber la media aritmética (el promedio) que es la\r\nsuma de todos los valores dividido por n. ¿Que hago?\r\n\r\n\r\ntotal<-sum(y)\r\nsum(y)\r\n\r\n\r\n[1] 43.03511\r\n\r\npero ahora necesito saber cuantos valores son\r\n\r\n\r\nn<-length(y)\r\nn\r\n\r\n\r\n[1] 39\r\n\r\nmedia<- total/n\r\nmedia\r\n\r\n\r\n[1] 1.103464\r\n\r\npero ahora quiero tener una función que me sirva para siempre\r\n\r\n\r\nmedia.aritmetica <- function(x) {\r\n  sum(x)/length(x) }\r\n\r\n\r\n\r\nya está, ahora probémosla\r\n\r\n\r\ndata<-c(3,4,6,7)\r\nmedia.aritmetica(data)\r\n\r\n\r\n[1] 5\r\n\r\n\r\n\r\nmedia.aritmetica(y)\r\n\r\n\r\n[1] 1.103464\r\n\r\n¡Que bien! ¡R es fantástico! puedo calcular la media siempre que me\r\nplazca.\r\nEn realidad la mayor parte de las funciones estadísticas están ya\r\nconstruidas en R. Por supuesto que la media es una de ellas. Solo quería\r\nmostrarles que no hay nada obscuro detrás de los objetos ya creados en\r\nR\r\n\r\n\r\nmean(y)\r\n\r\n\r\n[1] 1.103464\r\n\r\nLa media como medida de tendencia central tiene el serio problema de\r\nque es muy sensible a valores atípicos. vean lo siguiente.\r\n\r\n\r\ndataat<-c(data,100)\r\ndataat\r\n\r\n\r\n[1]   3   4   6   7 100\r\n\r\nmean(dataat)\r\n\r\n\r\n[1] 24\r\n\r\ncomparado con\r\n\r\n\r\nmean(data)\r\n\r\n\r\n[1] 5\r\n\r\nUna alternativa es la mediana, que es el valor de en medio, una vez\r\nque todos los valores han sido ordenados. Veamos dataat\r\n\r\n\r\ndataat\r\n\r\n\r\n[1]   3   4   6   7 100\r\n\r\n¿Cual es la mediana?\r\n\r\n\r\nmedian(dataat)\r\n\r\n\r\n[1] 6\r\n\r\nes mucho mejor estimación de el centro que 24.\r\n¿y de data?\r\n\r\n\r\ndata\r\n\r\n\r\n[1] 3 4 6 7\r\n\r\n\r\n\r\nmedian(data)\r\n\r\n\r\n[1] 5\r\n\r\n¿ y para y?\r\n\r\n\r\nmedian(y)\r\n\r\n\r\n[1] 1.108847\r\n\r\nmean(y)\r\n\r\n\r\n[1] 1.103464\r\n\r\nSe parecen mucho porque no hay valores atípicos y porque la\r\ndistribución es simétrica.\r\nAhora pensemos en fenómenos que cambian multiplicativamente. ¿Conocen\r\nalguno?\r\nUno de los más comunes en ecología es el crecimiento poblacional y\r\npor lo tanto la dispersión de organismos de una población. En dichos\r\ncasos la media aritmética y/o la mediana suelen ser pésimos estimadores\r\nde la tendencia central. Veamos un ejemplo.\r\nEl número de insectos en una serie de plantas vecinas es\r\n\r\n\r\ninsectos<-c(1,10,1000,10,1)\r\n\r\n\r\n\r\n¿Cual es la mejor estimación de tendencia central?\r\n\r\n\r\n#windows()\r\nhist(insectos)\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean(insectos)\r\n\r\n\r\n[1] 204.4\r\n\r\nmedian(insectos)\r\n\r\n\r\n[1] 10\r\n\r\nC.1\r\nR.2\r\nLo que se usa es la media geométrica que si se acuerdan hay dos\r\nmaneras de calcularla. Para el ejemplo de los insectos ¿cual es la mas\r\nsimple?\r\n\r\n\r\n100000^0.2\r\n\r\n\r\n[1] 10\r\n\r\n¿y la otra?\r\n\r\n\r\nexp(mean(log(insectos)))\r\n\r\n\r\n[1] 10\r\n\r\n\r\n\r\ndetach(yvals)\r\nrm(insectos)\r\nls()\r\n\r\n\r\n[1] \"data\"             \"dataat\"           \"media\"           \r\n[4] \"media.aritmetica\" \"n\"                \"total\"           \r\n[7] \"yord\"             \"yvals\"           \r\n\r\nC.2\r\nR.3\r\nMedidas de dispersión\r\nVeamos un cuerpo de datos cualquiera y preguntémonos cómo podemos\r\nmedir su dispersión.\r\n\r\n\r\ny<-c(13,7,5,12,9,15,6,11,9,7,12)\r\n\r\n\r\n\r\nveamos cómo se ve\r\n\r\n\r\n#windows()\r\nplot(y,ylim=c(0,20))\r\n\r\n\r\n\r\n\r\nLo más fácil es decir de donde a donde va (el intervalo).\r\n\r\n\r\nrange(y)\r\n\r\n\r\n[1]  5 15\r\n\r\nPero esto tiene sus problemas.\r\nNo tiene relación con el parámetro población de intervalo.\r\nIncrementa con la n.\r\nAdemás es muy susceptible a valores atípicos\r\nNo considera a todos lo valores.\r\nOtra medida de dispersión muy importante es la varianza. Que está\r\nfundamentada en las desviaciones (o residuales) de cada valor con la\r\nmedia\r\n\r\n\r\ny<-c(13,7,5,12,9,15,6,11,9,7,12)\r\nplot(y,ylim=c(0,20))\r\nabline(mean(y),0)\r\nfor (i in 1:11) lines(c(i,i),c(y[i],mean(y)))\r\n\r\n\r\n\r\n\r\nse usa la suma de cuadrados de la diferencia de cada valor con la\r\nmedia general como base. ¿Cómo lo calculamos?\r\n\r\n\r\ny-mean(y)\r\n\r\n\r\n [1]  3.3636364 -2.6363636 -4.6363636  2.3636364 -0.6363636  5.3636364\r\n [7] -3.6363636  1.3636364 -0.6363636 -2.6363636  2.3636364\r\n\r\n(y-mean(y))^2\r\n\r\n\r\n [1] 11.3140496  6.9504132 21.4958678  5.5867769  0.4049587 28.7685950\r\n [7] 13.2231405  1.8595041  0.4049587  6.9504132  5.5867769\r\n\r\nsum((y-mean(y))^2)\r\n\r\n\r\n[1] 102.5455\r\n\r\nFantásitico, pero que sucede a SC cada vez que yo adiciono una nueva\r\nobservación. ¿Que tenemos que hacer?\r\nSi divido entre n, se llama la desviación media de los cuadrados.\r\nC.3\r\nR.4\r\n\r\n\r\nvariance <- function (x)   sum((x-mean(x))^2)/(length(x)-1)\r\nvariance(y)\r\n\r\n\r\n[1] 10.25455\r\n\r\nPero claro, ya está definido.\r\n\r\n\r\nvar(y)\r\n\r\n\r\n[1] 10.25455\r\n\r\nLa relación entre la varianza de la muestra y el tamaño de muestra\r\n(n)\r\nLo que vamos a hacer es seleccionar aleatoriamente números de una\r\npoblación que tiene una distribución normal (media 10 y var 4). Esto lo\r\nvamos a hacer repetidas veces pero nuestra muestra va a ir incrementando\r\nsu n desde 3 hasta 31. Vamos a sacar 30 muestras de cada tamaño de\r\nmuestra. Es decir 30 muestras de 3 números, 30 muestras de 5 números\r\netc. A cada muestra le vamos a calcular su varianza y las vamos a\r\ngraficar.\r\n\r\n\r\n# windows()\r\nplot(c(0,32),c(0,15),type=\"n\",xlab=\"Tamaño de muestra\",ylab=\"Varianza\")\r\nfor (tm in seq(3,31,2)) {\r\nfor( i in 1:30){\r\nx<-rnorm(tm,mean=10,sd=2)\r\npoints(tm,var(x)) }}\r\n\r\n\r\n\r\n\r\nAhora pueden ver que la varianza poblacional puede estar muy mal\r\nestimada con tamaños de muestra pequeños. Y a medida que aumentamos el\r\ntamaño de muestra la probabilidad de que la estimación está muy lejos\r\ndel parámetro disminuye. Esta es una razón más para elegir muy\r\ncuidadosamente el tamaño de muestra. En unas clases vamos a hablar de\r\nesto en el contexto de pruebas de hipótesis.\r\nC.4\r\nR.5\r\nHagamos el mismo proceso que hicimos antes para elegir muestras con\r\ntamaños de muestra que incrementan pero ahora veamos que sucede con\r\nnuestra medida de desconfianza de la estimación a medida que aumenta\r\nn\r\n\r\n\r\n#windows()\r\nplot(c(0,32),c(0,2),type=\"n\",xlab=\"Tamaño de muestra\",ylab=\"Error estandar\")\r\n for (tm in seq(3,31,2)) {\r\nfor( i in 1:30){\r\nx<-rnorm(tm,mean=10,sd=2)\r\npoints(tm,sqrt(var(x)/tm)) }}\r\n\r\n\r\n\r\n\r\nVeamos un ejemplo de la concentración de ozono en unos\r\ninvernaderos\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar gardens\r\n\r\n\r\n\r\n\r\nozono<- read.table(\"gardens.txt\",header=T)\r\nattach(ozono)\r\nozono\r\n\r\n\r\n\r\n\r\n\r\n#windows()\r\npar(mfrow=c(1,3))\r\nplot (gardenA)\r\nplot (gardenB)\r\nplot (gardenC)\r\n\r\n\r\n\r\n\r\n\r\n\r\nMediaA<- mean(gardenA)\r\nEEA<- sqrt(var(gardenA)/10)\r\nMediaB<- mean(gardenB)\r\nEEB<- sqrt(var(gardenB)/10)\r\nMediaC<- mean(gardenC)\r\nEEC<- sqrt(var(gardenC)/10)\r\n\r\nMediaA\r\n\r\n\r\n[1] 3\r\n\r\nEEA\r\n\r\n\r\n[1] 0.3651484\r\n\r\nMediaB\r\n\r\n\r\n[1] 5\r\n\r\nEEB\r\n\r\n\r\n[1] 0.3651484\r\n\r\nMediaC\r\n\r\n\r\n[1] 5\r\n\r\nEEC\r\n\r\n\r\n[1] 1.19257\r\n\r\n¿Para cual invernadero puedo yo confiar más de la estimación de la\r\nmedia?\r\nC.5\r\nR.6\r\nVamos a calcular los intervalos de confianza para la media de los\r\ninvernaderos usando la distribución de t. qt nos d? el valor de t para\r\nel cual hay cierta proporción a la izquierda.\r\n\r\n\r\nqt(.975,9)\r\n\r\n\r\n[1] 2.262157\r\n\r\ncalculemos los intervalos de confianza al 95% para el invernadero\r\nB\r\n\r\n\r\nqt(.975,9)*sqrt(1.3333/10)\r\n\r\n\r\n[1] 0.8260127\r\n\r\nel reporte diario, la concentración promedio de ozono en el\r\ninvernadero B fue de 5.0?0.826(I.C.95%, n=10)\r\n\r\n\r\n#windows()\r\nplot(gardenB)\r\nabline(mean(gardenB),0)\r\nabline((mean(gardenB)+0.826),0, lty=2)\r\nabline((mean(gardenB)-0.826),0, lty=2)\r\n\r\n\r\n\r\n\r\nlos dejo que ustedes calculen aquellos de los invernaderos A y C.\r\nC.6\r\nFin ———————————————————————\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:47:38-06:00"
    },
    {
      "path": "Estimacion2bis.html",
      "title": "Script estimacion 2_bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntervalos de confianza\r\nC.6\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nIntervalos de confianza\r\nExiste otra manera enteramente distinta de estimar intervalos de\r\nconfianza, y esta es enteramente autoreferida. Se usa el método de\r\nremuestreo o bootstrapping. Veamos un cuerpo de datos.\r\n\r\n\r\ndata<-read.table(\"skewdata.txt\",header=T)\r\nattach(data)\r\nnames(data)\r\n\r\n\r\n\r\nvamos a verlos visual y gráficamente\r\n\r\n\r\nvalues\r\n\r\n\r\n [1] 81.372918 25.700971  4.942646 43.020853 81.690589 51.195236\r\n [7] 55.659909 15.153155 38.745780 12.610385 22.415094 18.355721\r\n[13] 38.081501 48.171135 18.462725 44.642251 25.391082 20.410874\r\n[19] 15.778187 19.351485 20.189991 27.795406 25.268600 20.177459\r\n[25] 15.196887 26.206537 19.190966 35.481161 28.094252 30.305922\r\n\r\n\r\n\r\nhist(values)\r\n\r\n\r\n\r\n\r\nAhora vamos a calcular los intervalos de confianza de remuestreo por\r\nquantiles a través de la función sample que remuestrea y la función\r\nquantile basados en la muestra. Ahora, la función construida creo que no\r\nexiste, así que aquí la definimos.\r\n\r\n\r\nRemuestreo<-function(x){\r\na<-numeric(10000)\r\nfor (i in 1:10000){\r\na[i]<-mean(sample(x,30,replace=T))}\r\nquantile(a,c(.025,.975))}\r\nRemuestreo(values)\r\n\r\n\r\n    2.5%    97.5% \r\n24.89675 37.86573 \r\n\r\nVeamos como comparan con los IC normales\r\n\r\n\r\nmean(values)+1.96*sqrt(var(values)/30)\r\n\r\n\r\n[1] 37.53846\r\n\r\nmean(values)-1.96*sqrt(var(values)/30) \r\n\r\n\r\n[1] 24.39885\r\n\r\nVean ustedes porque elegimos 30 como una n buena para tamaño de las\r\nmuestras repetidas\r\n\r\n\r\nplot(c(0,60),c(0,60),type=\"n\",xlab=\"Tamaño de muestra\",ylab=\"IC remuestreo\")\r\n \r\nfor (k in seq(5,60,3)){\r\na<-numeric(10000)\r\nfor (i in 1:10000){\r\na[i]<-mean(sample(values,k,replace=T))\r\n}\r\npoints(c(k,k),quantile(a,c(.025,.975)),type=\"b\")\r\n}\r\n\r\n# Ahora veamos como comparan con los valores de IC normales\r\nxv<-seq(5,60,0.1)\r\nyv<-mean(values)+1.96*sqrt(var(values)/xv)\r\nlines(xv,yv)\r\nyv<-mean(values)-1.96*sqrt(var(values)/xv)\r\nlines(xv,yv)\r\n\r\n#y los valores de IC de t student \r\n\r\nyv<-mean(values)-qt(.975,xv)*sqrt(var(values)/xv)\r\nlines(xv,yv,lty=2)\r\nyv<-mean(values)+qt(.975,xv)*sqrt(var(values)/xv)\r\nlines(xv,yv,lty=2)\r\n\r\n\r\n\r\n\r\nNoten que no son exactamente simétricos. Esto hace que sean\r\nligeramente sesgados. Existen otros dos métodos (por centiles y\r\nacelerados) que corrigen esto. También noten que para la distribución de\r\nt son más conservadores si el tamaño de muestra es pequeño.\r\nC.6\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:20:08-06:00"
    },
    {
      "path": "Explor_ggplot.html",
      "title": "Script Análisis exploratorios_ versión ggplot",
      "author": [],
      "contents": "\r\n\r\nContents\r\nLibrerías necesarias\r\nCargar datos\r\nBoxplots\r\nBoxplot\r\nbásico\r\nBoxplot\r\nggplot\r\n\r\nDotplots\r\nCon base R\r\nModo ggplot\r\n\r\nHistogramas\r\nBase R\r\nHistograma\r\nggplot\r\nHistograma de todos\r\n\r\nGráficos de densidad\r\nggplot de una variable\r\nggplot de todas las\r\nvariables juntas\r\n\r\nQ-Q plots\r\nggplot de\r\nqqplots\r\n\r\n\r\n\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el csv donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nLibrerías necesarias\r\n\r\n\r\nlibrary(tidyverse) # Manejo de datos y ggplot\r\nlibrary(ggrepel) # generar los labels de los ggplots\r\nlibrary(ggridges) # Compara distribuciones númericas\r\nlibrary(patchwork) # Permite unir varios gráficos de ggplot (también noggplot pero requiere más carpinteria)\r\n\r\n\r\n\r\nCargar datos\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\n\r\n\r\n\r\nBoxplots\r\nBoxplot básico\r\n\r\n\r\nboxplot(airpoll$SO2, ylab=\"SO2\")\r\n\r\n\r\n\r\n\r\nBoxplot ggplot\r\nPára hacer el gráfico necesitamos generar la base de datos en formato\r\n“largo”\r\n\r\n\r\nairpoll2 <- stack(airpoll)\r\n\r\n\r\n\r\n\r\nvalues\r\nind\r\n36\r\nRainfall\r\n35\r\nRainfall\r\n44\r\nRainfall\r\n47\r\nRainfall\r\n43\r\nRainfall\r\n53\r\nRainfall\r\n\r\n\r\n\r\nfilter(airpoll2, ind == \"SO2\") %>% #selecciono variable que me importa\r\n  ggplot(aes(x= ind, y= values))+ # Creo la capa de datos\r\n  geom_boxplot(outlier.color = \"red\", #color de outlier\r\n               fill= \"gray\")+ # color de la caja\r\n#  geom_text(aes(label= values), position = position_dodge2(0.3))+\r\n  labs(title = \" Boxplot de SO2\",\r\n       y= \" Valores\",\r\n       x= \"\")+\r\n  theme_bw() # el tema que gustes\r\n\r\n\r\n\r\n\r\nMás complejo pero con más capácidad de edición\r\nAhora si queremos gráficar todos los boxplors\r\nCon Base R es sencillo\r\n\r\n\r\nboxplot(airpoll)\r\n\r\n\r\n\r\nboxplot(log(airpoll+1)) # En escala log\r\n\r\n\r\n\r\n\r\nEn ggplot tenemos que usar la base de datos que tenemos en formato\r\n“long”\r\n\r\n\r\nggplot(airpoll2, aes(x=ind, y= values, fill=ind))+\r\n  geom_boxplot(outlier.colour = \"red\")+\r\n  #geom_jitter()+ # agregar puntos\r\n  labs(title = \"Boxplot Básico\", x= \"\", y= \"valores\")+\r\n  theme_bw()+\r\n  theme(legend.position = \"none\") #quitar leyenda de variables\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(airpoll2, aes(x=ind, y= log(values+1), fill=ind))+ #Ahora en log\r\n  geom_boxplot(outlier.colour = \"red\", notch = T)+\r\n  #geom_jitter()+ # agregar puntos\r\n  labs(title = \"Boxplot escala Log\", x= \"\", y= \"valores\")+\r\n  theme_bw()+\r\n  theme(legend.position = \"none\") #quitar leyenda de variables\r\n\r\n\r\n\r\n\r\nLos gráficos de violín pueden ser útiles también\r\n\r\n\r\nggplot(airpoll2, aes(x=ind, y= log(values+1), fill=ind))+\r\n  geom_violin()+ # geometría de violin\r\n  geom_jitter(aes(col= ind), alpha= 0.6)+\r\n    labs(title = \"Gráfico de violín\", \r\n         x= \"\", y= \"variables scala log\")+\r\n  theme_minimal()+\r\n  theme(legend.position = \"none\") #quitar leyenda de variables\r\n\r\n\r\n\r\n\r\nAhora en conjunto\r\n\r\n\r\nggplot(airpoll2, aes(x=ind, y= log(values+1)))+\r\n  geom_jitter( aes(color=ind),width = 0.2, alpha= 0.6)+\r\n  geom_violin(aes(color=ind),\r\n              scale = \"width\", \r\n              alpha= 0.1, # valor de opacidad\r\n              size= 0.8)+ # tamaño de la linea\r\n  geom_boxplot(width = 0.3, alpha= 0.1, size= 0.8,\r\n               outlier.color = \"red\",\r\n               outlier.alpha = 0.9,\r\n               outlier.size = 2)+\r\n  labs(title = \"Gráfico conjunto\", \r\n       x= \"\", y= \"Variables scala log\")+\r\n  theme_bw()+\r\n  theme(legend.position = \"none\", # quitar leyenda de variables\r\n       text = element_text(size= 11, face = \"bold\")) # Ajustar tamaño de texto\r\n\r\n\r\n\r\n\r\nMuchas más opciones en: https://www.r-graph-gallery.com/boxplot.html\r\nDotplots\r\nCon base R\r\n\r\n\r\ndotchart(airpoll$SO2, \r\n         ylab = \"Order of observations\", \r\n         xlab = \"SO2\", \r\n         main = \"Cleveland dotplot\")\r\n\r\n\r\n\r\n\r\nModo ggplot\r\n\r\n\r\nd_plot <-  airpoll %>% \r\n   mutate(index = seq(n())) %>%\r\n  select_if(is.numeric) %>% \r\n  pivot_longer(cols = !index, \r\n               names_to = \"Variable\", \r\n               values_to = \"Value\") %>%\r\n  ggplot(aes(x= Value, y= index, col= Variable))+\r\n  geom_point(size= 2, alpha=0.6)+\r\n  scale_color_viridis_d(option = \"mako\", \r\n                        begin = 0.1,\r\n                        end = 0.8)+\r\n  facet_wrap(~Variable, scales = \"free\")+\r\n  labs(y= \"Order of ovservation\")+\r\n   theme_bw()+\r\n   theme(legend.position = \"none\",\r\n         text = element_text(size=10))\r\n\r\n\r\n\r\nHistogramas\r\nBase R\r\n\r\n\r\nhist(airpoll$Rainfall,lwd=2)\r\n\r\n\r\n\r\n\r\nHistograma ggplot\r\n\r\n\r\nggplot(airpoll, aes(x=Rainfall))+\r\n  geom_histogram(bins =5, # número de breaks\r\n                 fill= \"gray\", color= \"black\")+\r\n  labs(title= \"Histograma\",\r\n       y= \"Frequency\")+\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nHistograma de todos\r\nTenemos que usar de nuevo nuestra tabla en formato largo\r\n\r\n\r\nggplot(airpoll2, aes(x=values, fill= ind))+\r\n  geom_histogram( bins = 7, color=\"black\")+\r\n  facet_wrap(.~ind, scales = \"free\")+ # Crear los paneles por variable\r\n  theme_bw()+\r\n  theme(legend.position = \"none\")+\r\n  labs(x=\"\", y= \" Frequency\")\r\n\r\n\r\n\r\n\r\nMas opciones en https://www.r-graph-gallery.com/histogram.html\r\nGráficos de densidad\r\nggplot de una variable\r\n\r\n\r\nggplot(airpoll, aes(x= Rainfall))+\r\n  geom_histogram(aes(y = stat(density)), \r\n                 bins = 7,\r\n                 fill= \"gray\",\r\n                 color=\"black\")+\r\n  geom_density(size=1)+\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nggplot de todas las\r\nvariables juntas\r\n\r\n\r\nggplot(airpoll2, aes(x=log(values+1), fill= ind))+\r\n  geom_histogram(aes(y = stat(density), fill= ind), \r\n                 bins = 7, color=\"black\")+\r\n  geom_density(aes(fill= ind),\r\n               size=1, alpha= 0.5)+\r\n  facet_wrap(.~ind, scales = \"free\")+ # Crear los paneles por variable\r\n  theme_classic()+\r\n  theme(legend.position = \"none\")+\r\n  labs(x=\"\", y= \" Frequency\")\r\n\r\n\r\n\r\n\r\nVariables a la misma escala con un gráfico de olas\r\n\r\n\r\nggplot(airpoll2, aes(x= log(values+1), y= ind, fill= ind))+\r\n  geom_density_ridges()+\r\n  theme_ridges() + \r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\n\r\nQ-Q plots\r\nVoy a crear una base con una columna que tenga el nombre de las\r\nciudades\r\n\r\n\r\nairpoll3 <- airpoll %>% \r\n  rownames_to_column(var= \"City\")\r\n\r\n\r\n\r\n###plot base\r\n\r\n\r\nqqnorm(airpoll$Education, main=\"Q-Q plot Education\")\r\nqqline(airpoll$Education, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\nCon los labels\r\n\r\n\r\nEducation_plot <- qqnorm(airpoll3$Education, main=\"Q-Q plot Education\")\r\nqqline(airpoll3$Education, col = 2, lty = 2)\r\ntext(Education_plot[[1]], Education_plot[[2]], airpoll3$City, pos = 3,\r\n     cex = 0.7)\r\n\r\n\r\n\r\n\r\nggplot de qqplots\r\n\r\n\r\nggplot(airpoll3, aes(sample=Education))+\r\n  stat_qq()+\r\n  stat_qq_line(linetype= \"dashed\")+\r\n  geom_text_repel(label= airpoll3$City[order(airpoll3$Education)], \r\n                  stat = \"qq\",\r\n                  size= 1.5)+\r\n  labs(title = \"Q-Q plot\", \r\n       x= \"Theoretical Quantiles\", \r\n       y= \"Education sample cuantiles\")+\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nGráfico conjunto Necesito que mi base de datos este en formato largo\r\njunto con la columna de Ciudad\r\n\r\n\r\nairpoll4 <- airpoll %>% \r\n  rownames_to_column(var= \"City\") %>% # de nombres a columna\r\n  pivot_longer(cols = c(names(airpoll)), #Las columnas que bajo\r\n               names_to = \"variable\", # Nombre de la nueva column\r\n               values_to = \"valores\") # Nombre de col de valores\r\n\r\n\r\n\r\n\r\nCity\r\nvariable\r\nvalores\r\nakronOH\r\nRainfall\r\n36.0\r\nakronOH\r\nEducation\r\n11.4\r\nakronOH\r\nPopden\r\n3243.0\r\nakronOH\r\nNonwhite\r\n8.8\r\nakronOH\r\nNOX\r\n15.0\r\nakronOH\r\nSO2\r\n59.0\r\n\r\n\r\n\r\nggplot(airpoll4, mapping = aes(sample= valores,colour= variable))+\r\n  stat_qq()+\r\n  stat_qq_line()+\r\n  labs(title = \"QQplots\",\r\n       x= \"Theoretical Quantiles\", \r\n       y= \"Sample cuantiles\"\r\n  )+\r\n  facet_wrap(.~variable , scales = \"free\")+\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nGráficos conjuntos señalando las ciudades Para este caso tube que\r\ncrear una función que aplicara el label para cada qqplot\r\n\r\n\r\nf_ggplot <- function(x){\r\n  airpoll3 <- airpoll3\r\n  n <- names(airpoll3)\r\n  p1 <- ggplot(airpoll3, aes(sample=x))+\r\n    stat_qq()+\r\n    stat_qq_line()+\r\n    geom_text_repel(label= airpoll3$City[order(x)], \r\n                    stat = \"qq\",\r\n                    size= 1.5)+\r\n    labs(title = \"QQplot\", \r\n         x= \"Theoretical Quantiles\", \r\n         y= \"Sample Quantile\")+\r\n    theme_bw()\r\n}\r\n\r\n\r\n\r\n\r\n\r\nlabel_plots <- map(airpoll3, f_ggplot)\r\n\r\n(label_plots[[2]]+ ggtitle(paste(names(label_plots[2]), \"QQplot\")))+\r\n  (label_plots[[3]]+ ggtitle(paste(names(label_plots[3]), \"QQplot\")))+\r\n  (label_plots[[4]]+ ggtitle(paste(names(label_plots[4]), \"QQplot\")))+\r\n  (label_plots[[5]]+ ggtitle(paste(names(label_plots[5]), \"QQplot\")))+\r\n  (label_plots[[6]]+ ggtitle(paste(names(label_plots[6]), \"QQplot\")))+\r\n  (label_plots[[7]]+ ggtitle(paste(names(label_plots[7]), \"QQplot\")))+\r\n  (label_plots[[8]]+ ggtitle(paste(names(label_plots[8]), \"QQplot\")))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T12:34:02-06:00"
    },
    {
      "path": "Exploratorios.html",
      "title": "Script Análisis exploratorios",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nExploración Univariada\r\nBoxplot\r\nHistogramas\r\nKurtosis\r\ny simetría\r\nQQplots\r\n\r\n\r\nRelaciones bivariadas\r\nC.1\r\nR.2\r\nvalores faltantes\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el csv donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nESTE EJEMPLO ES DE DATOS DE EVERITT 04 P. 17. CONSISTE EN\r\nINFORMACIÓN SOBRE CONTAMINACIÓN AMBIENTAL EN EU EN ZONAS\r\nMETROPOLITANAS.\r\nLlamo los datos (ojo que Everitt los tiene en formato dat, si están\r\ncomo txt, hay que llamarlos con read.table)\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\nap <- data.frame(airpoll)\r\n# write.csv(ap, \"apdf.csv\")\r\nadf <- read.csv(\"airpoll.csv\", header = T, sep = \";\") #tabla con dato faltante\r\n\r\n\r\n\r\n\r\n\r\nattach(airpoll)\r\nairpoll\r\n\r\n\r\n         Rainfall Education Popden Nonwhite NOX SO2 Mortality\r\nakronOH        36      11.4   3243      8.8  15  59     921.9\r\nalbanyNY       35      11.0   4281      3.5  10  39     997.9\r\nallenPA        44       9.8   4260      0.8   6  33     962.4\r\natlantGA       47      11.1   3125     27.1   8  24     982.3\r\nbaltimMD       43       9.6   6441     24.4  38 206    1071.0\r\nbirmhmAL       53      10.2   3325     38.5  32  72    1030.0\r\nbostonMA       43      12.1   4679      3.5  32  62     934.7\r\nbridgeCT       45      10.6   2140      5.3   4   4     899.5\r\nbufaloNY       36      10.5   6582      8.1  12  37    1002.0\r\ncantonOH       36      10.7   4213      6.7   7  20     912.3\r\nchatagTN       52       9.6   2302     22.2   8  27    1018.0\r\nchicagIL       33      10.9   6122     16.3  63 278    1025.0\r\ncinnciOH       40      10.2   4101     13.0  26 146     970.5\r\nclevelOH       35      11.1   3042     14.7  21  64     986.0\r\ncolombOH       37      11.9   4259     13.1   9  15     958.8\r\ndallasTX       35      11.8   1441     14.8   1   1     860.1\r\ndaytonOH       36      11.4   4029     12.4   4  16     936.2\r\ndenverCO       15      12.2   4824      4.7   8  28     871.8\r\ndetrotMI       31      10.8   4834     15.8  35 124     959.2\r\nflintMI        30      10.8   3694     13.1   4  11     941.2\r\nftwortTX       31      11.4   1844     11.5   1   1     891.7\r\ngrndraMI       31      10.9   3226      5.1   3  10     871.3\r\ngrnborNC       42      10.4   2269     22.7   3   5     971.1\r\nhartfdCT       43      11.5   2909      7.2   3  10     887.5\r\nhoustnTX       46      11.4   2647     21.0   5   1     952.5\r\nindianIN       39      11.4   4412     15.6   7  33     968.7\r\nkansasMO       35      12.0   3262     12.6   4   4     919.7\r\nlancasPA       43       9.5   3214      2.9   7  32     844.1\r\nlosangCA       11      12.1   4700      7.8 319 130     861.8\r\nlouisvKY       30       9.9   4474     13.1  37 193     989.3\r\nmemphsTN       50      10.4   3497     36.7  18  34    1006.0\r\nmiamiFL        60      11.5   4657     13.5   1   1     861.4\r\nmilwauWI       30      11.1   2934      5.8  23 125     929.2\r\nminnplMN       25      12.1   2095      2.0  11  26     857.6\r\nnashvlTN       45      10.1   2082     21.0  14  78     961.0\r\nnewhvnCT       46      11.3   3327      8.8   3   8     923.2\r\nneworlLA       54       9.7   3172     31.4  17   1    1113.0\r\nnewyrkNY       42      10.7   7462     11.3  26 108     994.6\r\nphiladPA       42      10.5   6092     17.5  32 161    1015.0\r\npittsbPA       36      10.6   3437      8.1  59 263     991.3\r\nportldOR       37      12.0   3387      3.6  21  44     894.0\r\nprovdcRI       42      10.1   3508      2.2   4  18     938.5\r\nreadngPA       41       9.6   4843      2.7  11  89     946.2\r\nrichmdVA       44      11.0   3768     28.6   9  48    1026.0\r\nrochtrNY       32      11.1   4355      5.0   4  18     874.3\r\nstlousMO       34       9.7   5160     17.2  15  68     953.6\r\nsandigCA       10      12.1   3033      5.9  66  20     839.7\r\nsanfrnCA       18      12.2   4253     13.7 171  86     911.7\r\nsanjosCA       13      12.2   2702      3.0  32   3     790.7\r\nseatleWA       35      12.2   3626      5.7   7  20     899.3\r\nspringMA       45      11.1   1883      3.4   4  20     904.2\r\nsyracuNY       38      11.4   4923      3.8   5  25     950.7\r\ntoledoOH       31      10.7   3249      9.5   7  25     972.5\r\nuticaNY        40      10.3   1671      2.5   2  11     912.2\r\nwashDC         41      12.3   5308     25.9  28 102     968.8\r\nwichtaKS       28      12.1   3665      7.5   2   1     823.8\r\nwilmtnDE       45      11.3   3152     12.1  11  42    1004.0\r\nworctrMA       45      11.1   3678      1.0   3   8     895.7\r\nyorkPA         42       9.0   9699      4.8   8  49     911.8\r\nyoungsOH       38      10.7   3451     11.7  13  39     954.4\r\n\r\nnames(airpoll)\r\n\r\n\r\n[1] \"Rainfall\"  \"Education\" \"Popden\"    \"Nonwhite\"  \"NOX\"      \r\n[6] \"SO2\"       \"Mortality\"\r\n\r\nExploración Univariada\r\nComenzamos por ver el vector de medias y varianzas\r\n#mean(airpoll) #sd(airpoll)^2\r\n\r\n\r\nsummary(airpoll)\r\n\r\n\r\n    Rainfall       Education         Popden        Nonwhite    \r\n Min.   :10.00   Min.   : 9.00   Min.   :1441   Min.   : 0.80  \r\n 1st Qu.:32.75   1st Qu.:10.40   1st Qu.:3104   1st Qu.: 4.95  \r\n Median :38.00   Median :11.05   Median :3567   Median :10.40  \r\n Mean   :37.37   Mean   :10.97   Mean   :3866   Mean   :11.87  \r\n 3rd Qu.:43.25   3rd Qu.:11.50   3rd Qu.:4520   3rd Qu.:15.65  \r\n Max.   :60.00   Max.   :12.30   Max.   :9699   Max.   :38.50  \r\n      NOX              SO2           Mortality     \r\n Min.   :  1.00   Min.   :  1.00   Min.   : 790.7  \r\n 1st Qu.:  4.00   1st Qu.: 11.00   1st Qu.: 898.4  \r\n Median :  9.00   Median : 30.00   Median : 943.7  \r\n Mean   : 22.65   Mean   : 53.77   Mean   : 940.4  \r\n 3rd Qu.: 23.75   3rd Qu.: 69.00   3rd Qu.: 983.2  \r\n Max.   :319.00   Max.   :278.00   Max.   :1113.0  \r\n\r\n\r\n\r\nsummary(airpoll$SO2)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   1.00   11.00   30.00   53.77   69.00  278.00 \r\n\r\nvease la diferencia entre la media y la mediana para reconocer\r\ndesviaciones, calculese el intervalo intercuartiles (3er-1er).\r\nBoxplot\r\n\r\n\r\n#windows()\r\nboxplot(SO2, range=0, ylab=\"SO2\", main= \"Range 0\") # en este caso, los \"bigotes\" del boxplot ubican el máximo (1) y el mínimo (278).\r\n\r\n\r\n\r\n\r\n\r\nboxplot(SO2, ylab=\"SO2\", main= \"Range 1.5\") #en este caso, la función se ejecuta con range = 1.5 por defecto.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\niqSO2<-69-11\r\niqSO2\r\n\r\n\r\n[1] 58\r\n\r\nUna buena regla de dedo para identificar datos atípicos es: que los\r\npuntos que caen mas allá del 3er+1.5(intercuartil) o mas bajo que\r\n1er-1.5(intercuartil) son valores atípicos.\r\n\r\n\r\natipicossup<-69+(iqSO2*1.5)\r\n  atipicossup\r\n\r\n\r\n[1] 156\r\n\r\n\r\n\r\natipicosinf<-abs(11-(iqSO2*1.5))\r\n  atipicosinf\r\n\r\n\r\n[1] 76\r\n\r\n\r\n\r\nhist(SO2,lwd=2)\r\nabline(v = 156, col = \"blue\")\r\n\r\n\r\n\r\n\r\n¿Cuales son los valores atípicos para SO2? Ahora veamos lo que\r\nconsidera R como atípicos por default\r\n\r\n\r\npar(mfrow=c(1,3))\r\nboxplot(SO2, range=0, ylab=\"SO2\", main= \"Range 0\")\r\nboxplot(SO2, ylab=\"SO2\", main=\"Por defecto\")\r\nboxplot(SO2, range=1.5, ylab=\"SO2\", main= \"Range = 1.5\")\r\n\r\n\r\n\r\n\r\nHistogramas\r\nVeamos las distribuciones de todas\r\n\r\n\r\npar(mfrow=c(3,3))\r\nhist(SO2,lwd=2); abline(v = c(53.77, 30), col = c(\"blue\", \"red\"))\r\nhist(Rainfall,lwd=2)\r\nhist(Education,lwd=2)\r\nhist(Popden,lwd=2)\r\nhist(Nonwhite,lwd=2)\r\nhist(NOX,lwd=2)\r\nhist(Mortality,lwd=2)\r\n\r\n\r\n\r\n\r\n¿reconocen desviaciones negativas o positivas? Son normales?\r\nKurtosis y simetría\r\nEn este caso se necesita el paquete datawizard (Patil et al. 2022) para correr las\r\npruebas de kurtosis y simetria\r\nPara instalarlo pueden usar\r\n\r\n\r\ninstall.packages(\"datawizard\")\r\n\r\n\r\n\r\n¿Cómo se ve sin sesgo y curtosis?\r\nSimetría (Skewness)\r\n\r\n\r\n\r\nlibrary(datawizard)\r\n\r\nnorm <- rnorm(1000) #Genero 1000 datos con distribución normal\r\n\r\nsk <- skewness(norm)\r\nsk\r\n\r\n\r\nSkewness |    SE\r\n----------------\r\n   0.080 | 0.077\r\n\r\n\r\n\r\nku <- kurtosis(norm)\r\nku\r\n\r\n\r\nKurtosis |    SE\r\n----------------\r\n  -0.089 | 0.154\r\n\r\n\r\n\r\nhist(norm, freq = F)\r\nlines(density(norm))\r\ntext(x= -2, y=0.3, label= paste(\"Skewness =\", round(sk$Skewness,2))) \r\ntext(x= 2, y=0.3, label= paste(\"Kurtosis =\",  round(ku$Kurtosis,2)))\r\n\r\n\r\n\r\n\r\nAhora con nuestros datos\r\n\r\n\r\nskewness(SO2)\r\n\r\n\r\nSkewness |    SE\r\n----------------\r\n   1.912 | 0.301\r\n\r\nkurtosis(SO2)\r\n\r\n\r\nKurtosis |    SE\r\n----------------\r\n   3.552 | 0.559\r\n\r\n\r\n\r\nhist(SO2,freq = F)\r\nlines(density(SO2))\r\ntext(x= 100, y=0.01, label= \"Skewness = 1.912\")\r\ntext(x= 250, y=0.01, label= \"Kurtosis = 3.552\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nskewness(log(SO2+1))\r\n\r\n\r\nSkewness |    SE\r\n----------------\r\n  -0.443 | 0.301\r\n\r\nkurtosis(log(SO2+1))\r\n\r\n\r\nKurtosis |    SE\r\n----------------\r\n  -0.390 | 0.559\r\n\r\n\r\n\r\nhist(log(SO2+1),freq = F)\r\nlines(density(log(SO2+1)))\r\ntext(x= 1, y=0.3, label= \"Skewness = -0.43\")\r\ntext(x= 5, y=0.3, label= \"Kurtosis = -0.45\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nskewness(Mortality)\r\n\r\n\r\nSkewness |    SE\r\n----------------\r\n   0.096 | 0.301\r\n\r\nkurtosis(Mortality)\r\n\r\n\r\nKurtosis |    SE\r\n----------------\r\n   0.156 | 0.559\r\n\r\n\r\n\r\nhist(Mortality,freq = F)\r\nlines(density(Mortality))\r\ntext(x= 800, y=0.005, label= \"Skewness = 0.096\")\r\ntext(x= 1100, y=0.005, label= \"Kurtosis = 0.156\")\r\n\r\n\r\n\r\n\r\nQQplots\r\n\r\n\r\npar(mfrow=c(3,3))                                                 \r\nqqnorm(SO2, main=\"Q-Q plot SO2\"); qqline(SO2, col = 2, lty = 2)\r\nqqnorm(Rainfall, main=\"Q-Q plot Rainfall\"); qqline(Rainfall, col = 2, lty = 2)\r\nqqnorm(Education, main=\"Q-Q plot Education\"); qqline(Education, col = 2, lty = 2)\r\nqqnorm(Popden, main=\"Q-Q plot Popden\"); qqline(Popden, col = 2, lty = 2)\r\nqqnorm(Nonwhite, main=\"Q-Q plot Nonwhite\"); qqline(Nonwhite, col = 2, lty = 2)\r\nqqnorm(NOX, main=\"Q-Q plot NOX\"); qqline(NOX, col = 2, lty = 2)\r\nqqnorm(Mortality, main=\"Q-Q plot Mortality\"); qqline(Mortality, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\nRelaciones bivariadas\r\nVeamos que relación hay entre las distintas variables. Aquí utilizo\r\nuna función smoooth (regresión con pesos locales) que permite sugerir\r\ncon los propios datos que tipo de relación pudieran tener.\r\n\r\n\r\npairs(airpoll, panel=panel.smooth)\r\n\r\n\r\n\r\n\r\nveamos con mas detalle la relación SO2-mortalidad\r\n\r\n\r\nnombres<-abbreviate(row.names(airpoll))\r\npar(mfrow=c(1,1))\r\nplot(SO2,Mortality,lwd=2,type=\"n\")\r\ntext(SO2,Mortality,labels=nombres,lwd=2)\r\n\r\n\r\n\r\n\r\n\r\n\r\ndetach(airpoll)\r\n\r\n\r\n\r\nC.1\r\nR.2 valores faltantes\r\n\r\n\r\nairpoldf <- read.table(\"datofalta.txt\")\r\nairpoldf\r\nattach(airpoldf)\r\n\r\n\r\n\r\nLo mas fácil la media\r\n\r\n\r\nsummary(Mortality)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n  789.7   892.4   943.7   936.6   977.1  1112.0       1 \r\n\r\n\r\n\r\nsum(is.na(Mortality))\r\n\r\n\r\n[1] 1\r\n\r\nCual es el valor imputado? Cuales son los problemas asociados a esta\r\nimputación?\r\nregresión mortalidad y SO2\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nplot(SO2,Mortality,lwd=2)\r\nabline(v = 59, h = 940.2)\r\nabline(v = 59, h = 921.9, col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nregmort<-lm(Mortality~SO2)\r\nsummary(regmort)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Mortality ~ SO2)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-126.625  -38.213   -7.796   35.582  196.528 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 915.4721     9.4932  96.435  < 2e-16 ***\r\nSO2           0.4266     0.1177   3.624  0.00062 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 57.47 on 57 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.1872,    Adjusted R-squared:  0.173 \r\nF-statistic: 13.13 on 1 and 57 DF,  p-value: 0.0006196\r\n\r\nm <-  (915.4720997 + (0.4266209*59))  \r\n\r\n\r\n\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nplot(SO2,Mortality,lwd=2)\r\nabline(v = 59, h = 940.2)\r\nabline(v = 59, h = 921.9, col = \"red\")\r\nabline(lm(Mortality~SO2))\r\n\r\n\r\n\r\n\r\n\r\n\r\npredict(regmort, list(SO2=58)) \r\n\r\n\r\n       1 \r\n940.2161 \r\n\r\n\r\n\r\nlogM<-log(Mortality)\r\nlogSO2<-log(SO2+7)\r\nloglog<-lm(logM~logSO2)\r\nsummary(loglog)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = logM ~ logSO2)\r\n\r\nResiduals:\r\n      Min        1Q    Median        3Q       Max \r\n-0.136793 -0.030759  0.000398  0.029763  0.212163 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 6.749926   0.021463 314.487  < 2e-16 ***\r\nlogSO2      0.026633   0.005928   4.493 3.48e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.05863 on 57 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.2615,    Adjusted R-squared:  0.2486 \r\nF-statistic: 20.19 on 1 and 57 DF,  p-value: 3.482e-05\r\n\r\n\r\n\r\npar(mfrow=c(1,2))\r\nplot(SO2,Mortality,lwd=2) \r\nabline(regmort)\r\nabline(v = 58, h = c(940.2161, 954.4211), col = \"red\")\r\n      \r\nplot(logSO2,logM,lwd=2) \r\nabline(lm(logM~logSO2))\r\nabline(v = 58, h = 940.2161, col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nplot(logSO2,logM,lwd=2) \r\nabline(lm(logM~logSO2))\r\n\r\n\r\n\r\n\r\nel valor de SO2 que corresponde al valor faltante de mortalidad es\r\n58. Como hemos generado un modelo de logaritmos a ambos lados de la\r\necuación sacamos el log del (SO2+7)\r\n\r\n\r\nlog(58+7)\r\n\r\n\r\n[1] 4.174387\r\n\r\nUsamos la función predict para predecir el valor correspondiente de\r\nMortalidad\r\n\r\n\r\npredict(loglog, list(logSO2=4.174387)) \r\n\r\n\r\n       1 \r\n6.861105 \r\n\r\npero recordando que usamos logaritmos en el modelo,\r\nretrotransformamos con el antilog con base e (e elevado al numero que\r\nnos interesa retro transformar)\r\n\r\n\r\nexp(6.861105)\r\n\r\n\r\n[1] 954.4211\r\n\r\nEl valor predicho por regresión lineal es? Cuales son los problemas\r\nasociados a esta imputación?\r\nFin\r\n\r\n\r\n\r\nPatil, Indrajeet, Dominique Makowski, Mattan S. Ben-Shachar, Brenton M.\r\nWiernik, Etienne Bacher, and Daniel Lüdecke. 2022.\r\n“Datawizard: An\r\nr Package for Easy Data Preparation and\r\nStatistical Transformations” 7: 4684. https://doi.org/10.21105/joss.04684.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T21:34:49-06:00"
    },
    {
      "path": "index.html",
      "title": "Introducción a la estadística inferencial",
      "description": "Página del curso en su versión 2023\n",
      "author": [
        {
          "name": "Simoneta Negrete Yankelevich",
          "url": "https://www.researchgate.net/profile/Simoneta-Negrete-Yankelevich"
        },
        {
          "name": "Carlos Cultid Medina",
          "url": "https://www.researchgate.net/profile/Carlos-Cultid-Medina"
        },
        {
          "name": "Gabriel Andrade Ponce",
          "url": "https://www.researchgate.net/profile/Gabriel-Andrade-Ponce?ev=hdr_xprf"
        }
      ],
      "date": "Enero 16, 2023",
      "contents": "\r\n\r\nContents\r\n¡Bienvenidos al curso!🙌\r\nCronograma del curso 📆\r\nMaterial de la clase 📚\r\nScripts de\r\nclase 📊\r\nAdicional\r\n\r\n\r\n¡Bienvenidos al curso!🙌\r\nEste es la página del curso de Introducción a la estadística\r\ninferencial, versión 2023.\r\n\r\nCronograma del curso 📆\r\n\r\nMaterial de la clase 📚\r\nEl material del curso está disponible en la página de classroom, en\r\nlas siguientes ligas\r\nÉl\r\npapel de la estadística en la investigación\r\nIntroducción\r\nal Lenguaje R\r\nEstimación\r\n1 y 2\r\nAnálisis\r\nexploratorios\r\nPrueba\r\nde hipótesis 1 y 2\r\nModelos\r\nlineales 1 y 2\r\nScripts de clase 📊\r\nLos scripts pueden ser visualizados en html en la parte superior\r\nderecha de esta página.\r\n\r\nRecuerden que las bases de datos se encuentran en el material de\r\nclase\r\nAdicional\r\nMonitor del curso 2023\r\nGabriel P. Andrade Ponce 📧 gabriel.andrade@posgrado.ecologia.edu.mx\r\nEdición de Scripts y página web: Gabriel Andrade\r\nPonce\r\nPágina hecha con la paquetería distill en RMarkdown\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T22:20:10-06:00"
    },
    {
      "path": "PanelCorrel.html",
      "title": "Panel de correlación",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPerformance Analyticis\r\n————————————————–\r\nCorrplot\r\n—————————————————————-\r\nggcorrplot\r\n————————————————————–\r\nGGally\r\n——————————————————————\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nPerformance Analyticis\r\n————————————————–\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\n\r\n\r\n\r\n\r\n\r\nlibrary(PerformanceAnalytics)\r\n\r\n\r\n\r\nchart.Correlation(log(airpoll+1),\r\n                  method=\"pearson\",\r\n                  histogram=TRUE,\r\n                  pch=20)\r\n\r\n\r\n\r\n\r\n?chart.Correlation\r\nCorrplot —————————————————————-\r\n\r\n\r\nlibrary(corrplot)\r\n\r\ncorr <- round(cor(log(airpoll+1), method = \"spearman\"),2)\r\ncor.mat <- cor.mtest(log(airpoll+1), conf.level = 0.95)\r\n\r\n\r\np1 <- corrplot(corr, method=\"color\",  \r\n         type=\"upper\", order=\"hclust\", \r\n         addCoef.col = \"black\", # Add coefficient of correlation\r\n         tl.col=\"black\", tl.srt=45, #Text label color and rotation\r\n         # Combine with significance\r\n         p.mat= cor.mat$p, sig.level = 0.01, insig = \"blank\", \r\n         # hide correlation coefficient on the principal diagonal\r\n         diag=FALSE \r\n)$corrPos      \r\ntext(p1$x, p1$y, round(p1$corr, 2))\r\n\r\n\r\n\r\n\r\nggcorrplot ————————————————————–\r\n\r\n\r\nlibrary(ggcorrplot)\r\n         \r\nggcorrplot(corr, \r\n           type = \"lower\",\r\n           lab = T, show.diag = F, \r\n           legend.title = \" Pearson\\nCorrelation\", \r\n           colors= c(\"#BB4444\", \"#FFFFFF\", \"#4477AA\"), \r\n           hc.order = T, \r\n           sig.level = 0.05, insig = \"pch\", pch=8, pch.cex = 2,  \r\n           p.mat= cor.mat$p, \r\n           ggtheme = ggplot2::theme(\r\n            panel.background = element_blank()))  \r\n\r\n\r\n\r\n\r\nGGally ——————————————————————\r\n\r\n\r\nlibrary(GGally)\r\n\r\npairs <- ggpairs(log(airpoll+1),\r\n        upper = list(continuous= wrap(\"cor\", method= \"pearson\", digits=2)),\r\n        lower = list( continuous= \"smooth\")) +theme_classic()\r\n\r\npairs\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:28:00-06:00"
    },
    {
      "path": "Phdospob.html",
      "title": "Script Prueba de hipótesis de dos poblaciones",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.3\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.3\r\nVolvamos al ejemplo de las concentraciones de ozono en los\r\ninvernaderos y vamos a preguntarnos si el promedio de sus\r\nconcentraciones de ozono es significativamente distinto\r\n\r\n\r\nozono<-read.table(\"gardens.txt\",header=T)\r\nattach(ozono)\r\nnames(ozono)\r\n\r\n\r\n\r\nVeamos un gráfico\r\n\r\n\r\nozonoAB<-c(gardenA,gardenB)\r\nozonoAB\r\n\r\n\r\n [1] 3 4 4 3 2 3 1 3 5 2 5 5 6 7 4 4 3 5 6 5\r\n\r\netiqueta<-factor(c(rep(\"A\",10),rep(\"B\",10)))\r\netiqueta\r\n\r\n\r\n [1] A A A A A A A A A A B B B B B B B B B B\r\nLevels: A B\r\n\r\n\r\n\r\nboxplot(ozonoAB~ etiqueta, notch=T,\r\n        xlab=\"Invernadero\",ylab=\"Ozono\")\r\n\r\n\r\n\r\n\r\nSi usamos el ojimetro, parece ser que no sus medianas no son\r\ndistintas porque los intervalos intercuartil no se sobrelapan. Ahora\r\nhagamos una prueba de t (para comparar las medias) a pie.\r\n¿Que necesitamos?\r\nlos grados de libertad. dijimos que para una prueba de dos\r\npoblaciones calculamos el total de observaciones (20) menos el num. de\r\nparámetros estimados antes de realizar la prueba (dos medias). Así que\r\ntenemos 18 g.l.\r\nNecesitamos calcualar las varianzas individuales de cada\r\ninvernadero, para poder calcular la diferencia de EE.\r\n\r\n\r\ns2A<-var(gardenA)\r\ns2B<-var(gardenB)\r\n\r\n\r\n\r\n¿Luego que sigue?\r\nCalculamos la t de student para la diferencia de medias\r\n\r\n\r\n(mean(gardenA)-mean(gardenB))/sqrt(s2A/10+s2B/10)\r\n\r\n\r\n[1] -3.872983\r\n\r\nNoten que podemos ignorar el signo de la t. Porque solo depende que\r\ncual media pusimos primero. El valor absoluto es lo que importa.\r\nAhora necesitamos el valor crítico de la distribución de t de\r\nreferencia para determinar si aceptamos o rechazamos la hipótesis de que\r\nno hay diferenicas entre medias (que vienen de la misma población). Se\r\ntrata de un problema de una o dos colas?\r\nComo no me interesa cual es mayor o menor, ni tengo ninguna hipótesis\r\nde que invernadero debía tener mas o menos ozono, entonces es de dos\r\ncolas, por lo tanto uso una probabilidad de?\r\n\r\n\r\nqt(0.975,18)\r\n\r\n\r\n[1] 2.100922\r\n\r\n¿Acepto o rechazo la hipótesis de que son iguales?\r\nEn virtud de que el valor calculado es mayor que el valor crítico se\r\nrechaza la hipótesis nula. (recuerden más alto=rechazo Ho más\r\nbajo=acepto)\r\nFinalmente necesito saber cual es la probabilidad de que encuentre yo\r\nla diferencia entre estas dos muestras (o una más extrema) a pesar de\r\nque provienen de poblaciones con la misma media. Recuerden que es un\r\nproblema de dos colas y por esa razón necesito calcular pt y después\r\nmultiplicarlos por dos (para los dos extremos).\r\n\r\n\r\np<-2*pt(-3.872983,18)\r\np\r\n\r\n\r\n[1] 0.00111454\r\n\r\npara calcular los intervalos de confianza para la diferencia\r\nmedias\r\n\r\n\r\ndifmed<-mean(gardenA)-mean(gardenB)\r\nEEdifmed<-sqrt(s2A/10+s2B/10)\r\nt.de.tablas.alfa.05.gl.18<-qt(0.975,18)\r\nt.de.tablas.alfa.05.gl.18\r\n\r\n\r\n[1] 2.100922\r\n\r\n\r\n\r\nIC95<-(EEdifmed)*(t.de.tablas.alfa.05.gl.18)\r\nCotasup<- difmed+IC95\r\nCotasup\r\n\r\n\r\n[1] -0.9150885\r\n\r\n\r\n\r\nCotainf<- difmed-IC95 \r\nCotainf\r\n\r\n\r\n[1] -3.084911\r\n\r\n\r\n\r\nIC95t<-(sqrt(s2A/10+s2B/10))*(qt(0.975,18))\r\n  IC95t\r\n\r\n\r\n[1] 1.084911\r\n\r\n¿Cual es la probabilidad de que suceda? Ahora el automático\r\n\r\n\r\npruebat<-t.test(gardenA,gardenB)\r\npruebat\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  gardenA and gardenB\r\nt = -3.873, df = 18, p-value = 0.001115\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -3.0849115 -0.9150885\r\nsample estimates:\r\nmean of x mean of y \r\n        3         5 \r\n\r\nEntonces reporto la concentración de ozono fue significativamente más\r\nalta en el invernadero B (5.0 ppm) que en el A(3.0ppm;\r\nt=3.87, p=0.001(dos colas),\r\ngl=18)\r\nAhora, pudiéramos tener el problema de que las varianzas no son\r\niguales. entonces antes de hacer una prueba de t, necesitamos\r\npreguntarnos si las varianzas son significativamente distintas.\r\nComparemos las de los invernaderos B y C.\r\n\r\n\r\nvar(gardenB)\r\n\r\n\r\n[1] 1.333333\r\n\r\nvar(gardenC)\r\n\r\n\r\n[1] 14.22222\r\n\r\n¿Se acuerdan que la distribución de F era la adecuada para comparar\r\nvarianzas? la pregunta es, ¿cual es la probabilidad de que estas dos\r\nmuestras hayan sido sacada de poblaciones que tienen la misma varianza?\r\nEl valor de F es simplemente el cociente de las varianzas\r\n\r\n\r\nCocienteF<-var(gardenC)/var(gardenB)\r\nCocienteF\r\n\r\n\r\n[1] 10.66667\r\n\r\nAhora comparo con la distribución de probabilidad de F para los\r\ngrados de libertad correspondientes, y como no tengo ninguna idea de\r\ncual de las muestras debiera tener una varianza más alta, entonces es\r\nuna prueba de dos colas.\r\nComo F no es simétrica, necesito calcular ambos lados.\r\n\r\n\r\nqf(0.975,9,9)\r\n\r\n\r\n[1] 4.025994\r\n\r\nqf(0.025,9,9)\r\n\r\n\r\n[1] 0.2483859\r\n\r\n¿Que concluyo?\r\n¿Cual es la probabilidad de que estas dos muestras provengan de\r\npoblaciones con la misma varianza?\r\n\r\n\r\n2*(1-pf(CocienteF,9,9))\r\n\r\n\r\n[1] 0.001624199\r\n\r\nAhora el automático\r\n\r\n\r\nvar.test(gardenB,gardenC)\r\n\r\n\r\n\r\n    F test to compare two variances\r\n\r\ndata:  gardenB and gardenC\r\nF = 0.09375, num df = 9, denom df = 9, p-value = 0.001624\r\nalternative hypothesis: true ratio of variances is not equal to 1\r\n95 percent confidence interval:\r\n 0.02328617 0.37743695\r\nsample estimates:\r\nratio of variances \r\n           0.09375 \r\n\r\nAhora, si encontráramos que suponer que las poblaciones de las que\r\nprovienen las muestras no se distribuyen de manera normal entonces\r\ntenemos la alternativa de la prueba de Wilcoxon de suma de rangos. Esta\r\nes idéntica en su sistema a la de los rangos signados. Veamos como\r\nLo primero que hago es poner todas las observaciones en un mismo\r\nvector\r\n\r\n\r\nozone<-c(gardenA,gardenB)\r\nozone\r\n\r\n\r\n [1] 3 4 4 3 2 3 1 3 5 2 5 5 6 7 4 4 3 5 6 5\r\n\r\nAhora les hago sus etiquetas para que no se me pierdan\r\n\r\n\r\netiqueta<-c(rep(\"A\",10),rep(\"B\",10))\r\netiqueta\r\n\r\n\r\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\r\n[17] \"B\" \"B\" \"B\" \"B\"\r\n\r\nAhora hago un vector de los rangos con la función rank\r\n\r\n\r\nrangoscomb<-rank(ozone)\r\nrangoscomb\r\n\r\n\r\n [1]  6.0 10.5 10.5  6.0  2.5  6.0  1.0  6.0 15.0  2.5 15.0 15.0 18.5\r\n[14] 20.0 10.5 10.5  6.0 15.0 18.5 15.0\r\n\r\nNoten que para todos los valores repetidos se hace un promedio de los\r\nrangos que les tocan.\r\n\r\n\r\ntapply(rangoscomb,etiqueta,sum)\r\n\r\n\r\n  A   B \r\n 66 144 \r\n\r\nFinalmente uso el valor más pequeño (66) para compararlo con el valor\r\nde tablas para el etadístico W para n de 10 y 10 y un alfa del 5%\r\n(W=78). Como nuestro valor es menor\r\n(porque uso como referencia el mas pequeño del par que obtuve), entonces\r\nrechazo mi H0. Las Medias son significativamente distintas.\r\nAhora el automático.\r\n\r\n\r\nwilcox.test(gardenA,gardenB)\r\n\r\n\r\n\r\n    Wilcoxon rank sum test with continuity correction\r\n\r\ndata:  gardenA and gardenB\r\nW = 11, p-value = 0.002988\r\nalternative hypothesis: true location shift is not equal to 0\r\n\r\nLas diferencias se deben a que R utiliza un algoritmo de aproximación\r\npara calcular valores de z. Es ligeramente distinto del tradicional que\r\nhicimos arriba. La mecánica es la misma. Noten la diferencia en las p de\r\nt y W. Wilcoxon es menos poderoso (95%). Pero es el correcto si las\r\ndistribuciones son sesgadas. También es correcto si no lo son (aunque\r\npoco menos poderoso). t no es correcta si hay desviaciones sustanciales\r\nde la normalidad.\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:35:32-06:00"
    },
    {
      "path": "Phdospobbis.html",
      "title": "Script Prueba de hipótesis de dos poblaciones-bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.3\r\nC.3\r\nTarea\r\nAquí va la segunda parte de su tarea\r\n\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.3\r\nEntonces ¿cual es la probabilidad de que mi muestra de frecuencias (o una mas extrema) pertenezca a una población donde el aislamiento de la planta y el plumaje del colibrí defensor son independientes?\r\n\r\n\r\n1-pchisq(35.34,1)\r\n\r\n\r\n[1] 2.768866e-09\r\n\r\n¿Cual es el valor crítico para rechazar la H0 con un alfa de 0.05?\r\n\r\n\r\nqchisq(0.95,1)\r\n\r\n\r\n[1] 3.841459\r\n\r\nAhora vamos a hacerlo de manera automática en R\r\n\r\n\r\ncolibries<-matrix(c(38,14,11,51),nrow=2)\r\ncolibries\r\n\r\n\r\n     [,1] [,2]\r\n[1,]   38   11\r\n[2,]   14   51\r\n\r\nchisq.test(colibries)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test with Yates' continuity correction\r\n\r\ndata:  colibries\r\nX-squared = 33.112, df = 1, p-value = 8.7e-09\r\n\r\nNoten que estos valores son un poco distintos porque se aplicó una corrección de Yates. Esta fue diseñada para frecuencias pequeñas, pero ahora existen pruebas mejores para frecuencias pequeñas (20% o más frecuencias menores a 5) como La Prueba exacta de Fisher Crawley p. 90 QyK p.388.\r\nEntonces le quito la corrección\r\n\r\n\r\nchisq.test(colibries,correct=F)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  colibries\r\nX-squared = 35.334, df = 1, p-value = 2.778e-09\r\n\r\nme da exactamente lo que calculamos a mano.\r\nC.3\r\nTarea\r\nAquí va la segunda parte de su tarea\r\nEl problema se trata de un investigador que esta interesado en si las hormigas construyen preferentemente sus nidos en árboles de una de dos especies. Entonces muestreó 100 árboles de cada una de las especies. Encontró que 60 árboles de la especie A y 20 de la especie B tenían nidos. ¿Esta muestra apoya la hipótesis de preferencia?\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:37-06:00"
    },
    {
      "path": "Phunapob.html",
      "title": "Script Prueba de hipótesis de una poblacion",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nC.1\r\nR.2\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nVeamos un ejemplo de prueba de hipótesis para una población (Crawley\r\np.64). Estos son datos de Michelson (1978), de medidas tomadas para\r\nestimar la velocidad de la luz. Que ahora sabemos es cercano (299,990 km\r\np seg).\r\n\r\n\r\nveluz<-read.table(\"light.txt\",header=T)\r\nattach(veluz)\r\nnames(veluz)\r\n\r\n\r\n\r\n\r\n\r\nhist(speed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(speed)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n    650     850     940     909     980    1070 \r\n\r\nA todos los valores se les ha restado 299 000 para facilitar su\r\nvisualización ¿que pueden ver aquí? ¿tenemos valores atípicos?\r\n\r\n\r\nboxplot(speed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nqqnorm(speed)\r\nqqline(speed,lty=2)\r\n\r\n\r\n\r\n\r\nLa muestra no se distribuye de acuerdo a lo esperado normalmente\r\n(cosa que una prueba de t para una población asume), pero además ¿Creen\r\nque la población de referencia se distribuya de manera normal?\r\nNo puede porque tiene el problema de que la vel de la luz no puede\r\ntomar valores negativos\r\nNuestra hipótesis es que los datos de Michelson difieren de el valor\r\nprevaleciente en esa época como la vel de la luz 299,990 km/seg. Como a\r\ntodos los valores les quitaron 299 000 entonces ¿cual va a ser la\r\nreferencia?\r\nAhora, si cumpliéramos con el supuesto de que la pob. de referencia\r\nse distribuye normalmente, cómo resolvemos este problema?…ustedes ya lo\r\nsaben hacer.\r\nPrimero. ¿Este problema tiene una o dos colas que le pisen? Entonces\r\n¿que harían?\r\nBueno, pero como no cumplimos con el primero de los supuestos,\r\nnecesitamos otra alternativa. Que se les ocurre?\r\nPor supuesto que también podemos usar una técnica de remuestreo con\r\nremplazo! Este es el punto 1. de la tarea de hoy. Hagan la\r\nprueba con bootstrap\r\nExiste otra alternativa es una prueba llamada de rangos signados de\r\nWilcoxon.\r\nC.1\r\nR.2\r\n\r\n\r\nlibrary(stats)\r\n\r\nwilcox.test(speed,mu=990)\r\n\r\n\r\n\r\n    Wilcoxon signed rank test with continuity correction\r\n\r\ndata:  speed\r\nV = 22.5, p-value = 0.00213\r\nalternative hypothesis: true location is not equal to 990\r\n\r\nLa probabilidad de obtener la media de nuestra muestra (estadístico)\r\nen una población de las medias de muchas muestras con media 990\r\n(parámetro) es del 0.2%. Como aceptamos una probabilidad de equivocarnos\r\nal rechazar una H0 cuando esta es cierta del 5% entonces, la\r\nrechazamos!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:32:12-06:00"
    },
    {
      "path": "Phunapobbis.html",
      "title": "Script Prueba de hipótesis de una poblacion-bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nC.2\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece una\r\npantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nC.2\r\nVamos a escribir una función para calcular sesgo.\r\n\r\n\r\nskew<-function(x){\r\nm3<-sum((x-mean(x))^3)/length(x)\r\ns3<-sqrt(var(x))^3\r\nm3/s3  }\r\n\r\n\r\n\r\nveamos los datos skewdata\r\n\r\n\r\ndata<-read.table(\"skewdata.txt\",header=T)\r\nattach(data)\r\nnames(data)\r\n\r\n\r\n\r\n\r\n\r\nhist(values)\r\n\r\n\r\n\r\n\r\nCalculemos el sesgo\r\n\r\n\r\nskew(values)\r\n\r\n\r\n[1] 1.318905\r\n\r\nAhora hagamos una prueba de t para determinar si hay sesgo respecto a\r\nlo esperado para una población de inferencia con distribución normal\r\n\r\n\r\nskew(values)/sqrt(6/length(values))\r\n\r\n\r\n[1] 2.949161\r\n\r\n1-pt(2.949,28)\r\n\r\n\r\n[1] 0.003185136\r\n\r\n¿Cual es la conclusión?. Efectivamente está más sesgado de los\r\nesperado normalmente, ¿que se puede hacer, si insistimos en cumplir con\r\nla normalidad?\r\n\r\n\r\nskew(sqrt(values))/sqrt(6/length(values))\r\n\r\n\r\n[1] 1.474851\r\n\r\nskew(log(values))/sqrt(6/length(values))\r\n\r\n\r\n[1] -0.6600605\r\n\r\n\r\n\r\nkurtosis<-function(x) {\r\nm4<-sum((x-mean(x))^4)/length(x)\r\ns4<-var(x)^2\r\nm4/s4 - 3  }\r\nkurtosis(values)\r\n\r\n\r\n[1] 1.297751\r\n\r\nkurtosis(values)/sqrt(24/length(values))\r\n\r\n\r\n[1] 1.45093\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:32:23-06:00"
    },
    {
      "path": "Regresionsimp.html",
      "title": "Script Modelos lineales",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nC.1\r\nR.2\r\nC.2\r\nR.3\r\nC.3\r\nR.4\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nR.1\r\nLlamo los datos, los coloco en un dataframe y convierto las columnas\r\nen variables\r\n\r\n\r\nreg.data<-read.table(\"tannin.txt\",header=T)\r\nattach(reg.data)\r\nnames(reg.data)\r\n\r\n\r\n\r\nGráfico\r\n\r\n\r\npar(mfrow=c(1,1))\r\nplot(tannin,growth,pch=16)\r\n\r\n\r\n\r\n\r\n¿La tendencia de la variable de respuesta es a incrementar o a\r\ndisminuir con la explicatoria? tendencia a disminuir\r\n¿Es factible que los datos sean explicados por una línea horizontal?\r\nH0.\r\n\r\n\r\nplot(tannin,growth,pch=16)\r\nabline(mean(growth),0)\r\n\r\n\r\n\r\n\r\nLa H0 no parece factible, entonces b es probablemente dif de 0 y\r\nnegativa\r\n¿Si existe una tendencia es recta o curva? relación recta, entonces\r\nproponemos el modelo \\(y=a+bx+e\\)\r\n¿La dispersión de los datos es uniforme a lo largo de la línea o\r\ncambia sistemáticamente con la variable explicatoria?. Dispersión muy\r\nuniforme, la ordenada al origen es dif de 0 entonces a es prob mayor que\r\n0.\r\n¿A ojo cuales son los valores de a y b? Como podemos hacer este\r\nproceso sistemático y preciso?\r\nC.1\r\nR.2\r\nY la variación total de y es la dispersión de los datos alrededor de\r\ny barra.\r\nLa Suma de Cuadrados Total es \\(SCT=\r\n\\sum(y-\\overline{y})^2\\)\r\n\r\n\r\nplot(tannin,growth,pch=16)\r\nabline(mean(growth),0)\r\nfor (i in 1:9) lines(c(tannin[i],tannin[i]),c(growth[i],mean(growth)))\r\n\r\n\r\n\r\n\r\nLa mejor recta ajustada por el método de mínimos cuadrados es aquella\r\nque minimiza la Suma de Cuadrados de las desviaciones de los valores de\r\ny de la línea ajustada \\(\\hat{y}\\),\r\n\\(SCE=\\sum(y - \\hat{y})^2\\)\r\n\r\n\r\nplot(tannin,growth,pch=16) \r\nabline(lm(growth~tannin))\r\n\r\n\r\n\r\n\r\n\r\n\r\nysomb <- predict(lm(growth ~ tannin))\r\nplot(tannin,growth,pch=16) \r\nabline(lm(growth~tannin))\r\nfor(i in 1:9) lines(c(tannin[i], tannin[i]), c(growth[i], ysomb[i]))\r\n\r\n\r\n\r\n\r\nC.2\r\nR.3\r\nAhora bien, una tercera cantidad es la Suma de Cuadrados de la\r\nRegresión (es decir del efecto de la variable predictora)\r\n\\[SCR = SCTotal - SCError\\]\r\n\r\n\r\nplot(tannin, growth, type = \"n\") \r\nabline(mean(growth), 0) \r\nmodelito <- lm(growth ~ tannin) \r\nabline(modelito) \r\nfor(i in 1:9) lines(c(tannin[i], tannin[i]), c(mean(growth), predict(modelito)[i])) \r\npoints(tannin,predict(modelito), pch = 16) \r\npoints(tannin, growth)\r\n\r\n\r\n\r\n\r\nC.3\r\nR.4\r\nEmpezamos a ajustar los modelos: modelo nulo - solo la media\r\n\r\n\r\nNulo <- lm(growth ~ 1) \r\nnames(Nulo)\r\n\r\n\r\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \r\n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \r\n [9] \"call\"          \"terms\"         \"model\"        \r\n\r\n\r\n\r\nanova(Nulo) \r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: growth\r\n          Df Sum Sq Mean Sq F value Pr(>F)\r\nResiduals  8 108.89  13.611               \r\n\r\n\r\n\r\nNulo$df.residual \r\n\r\n\r\n[1] 8\r\n\r\nNulo$coefficients \r\n\r\n\r\n(Intercept) \r\n   6.888889 \r\n\r\nNulo$fitted.values\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n6.888889 6.888889 6.888889 6.888889 6.888889 6.888889 6.888889 \r\n       8        9 \r\n6.888889 6.888889 \r\n\r\n\r\n\r\nplot(tannin, growth) \r\nabline(a=Nulo$coe, b=0) \r\nabline(Nulo$coe, 0)\r\n\r\n\r\n\r\n\r\nEs decir solo se ha ajustado la media que no ofrece información\r\nimportante\r\nAgregamos el efecto del tannin\r\n\r\n\r\nTanino <- update(Nulo, . ~ . + tannin)\r\nTanino\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nCoefficients:\r\n(Intercept)       tannin  \r\n     11.756       -1.217  \r\n\r\n\r\n\r\nanova(Tanino)\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: growth\r\n          Df Sum Sq Mean Sq F value    Pr(>F)    \r\ntannin     1 88.817  88.817  30.974 0.0008461 ***\r\nResiduals  7 20.072   2.867                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nTanino$coefficients\r\n\r\n\r\n(Intercept)      tannin \r\n  11.755556   -1.216667 \r\n\r\n\r\n\r\nsummary(Tanino)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4556 -0.8889 -0.2389  0.9778  2.8944 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.7556     1.0408  11.295 9.54e-06 ***\r\ntannin       -1.2167     0.2186  -5.565 0.000846 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.693 on 7 degrees of freedom\r\nMultiple R-squared:  0.8157,    Adjusted R-squared:  0.7893 \r\nF-statistic: 30.97 on 1 and 7 DF,  p-value: 0.0008461\r\n\r\nO bien pedimos la secuencia de ajustes, que produce estos cambios en\r\ndevianza\r\n\r\n\r\nanova(Nulo, Tanino)\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: growth ~ 1\r\nModel 2: growth ~ tannin\r\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \r\n1      8 108.889                                  \r\n2      7  20.072  1    88.817 30.974 0.0008461 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nsummary(Tanino)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4556 -0.8889 -0.2389  0.9778  2.8944 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.7556     1.0408  11.295 9.54e-06 ***\r\ntannin       -1.2167     0.2186  -5.565 0.000846 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.693 on 7 degrees of freedom\r\nMultiple R-squared:  0.8157,    Adjusted R-squared:  0.7893 \r\nF-statistic: 30.97 on 1 and 7 DF,  p-value: 0.0008461\r\n\r\nSi queremos, podemos guardar los valores ajustados y los residuales\r\nen la base de datos:\r\n\r\n\r\nreg.data$ajustados <- fitted.values(Tanino) \r\nreg.data$residuales <- residuals(Tanino)\r\nreg.data\r\n\r\n\r\n  growth tannin ajustados residuales\r\n1     12      0 11.755556  0.2444444\r\n2     10      1 10.538889 -0.5388889\r\n3      8      2  9.322222 -1.3222222\r\n4     11      3  8.105556  2.8944444\r\n5      6      4  6.888889 -0.8888889\r\n6      7      5  5.672222  1.3277778\r\n7      2      6  4.455556 -2.4555556\r\n8      3      7  3.238889 -0.2388889\r\n9      3      8  2.022222  0.9777778\r\n\r\nPara inspeccionar qué tan bueno es el modelo existen algunos recursos\r\ngráficos donde se examinan la distribución de los residuales y los\r\npuntos extremos que que pueden “cargar” el valor numérico de los\r\nparámetros:\r\n\r\n\r\npar(mfcol=c(2,2))\r\nplot(Tanino)\r\n\r\n\r\n\r\n\r\nExtra\r\nUna función muy interesante para evaluar un modelo es\r\ncheck_model del paquete performance(Lüdecke et al. 2021). Recuerda que\r\npuedes instalarlo así:\r\n\r\n\r\n# Debido a que performance requiere del paquete see para graficar, es bueno instalar ambos\r\ninstall.packages(c(\"performance\", \"see\"))\r\n\r\n\r\n\r\nCargamos las librerias\r\n\r\n\r\nlibrary(performance)\r\nlibrary(see)\r\n\r\n\r\n\r\nAhora usamos la función\r\n\r\n\r\ncheck_model(Tanino)\r\n\r\n\r\n\r\n\r\nSi son fans de el diagnóstico numérico:\r\n\r\n\r\n# Por defecto usa los residuales estandarizados\r\ncheck_normality(Tanino)\r\n\r\n\r\nOK: residuals appear as normally distributed (p = 0.990).\r\n\r\nshapiro.test(rstandard(Tanino)) \r\n\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  rstandard(Tanino)\r\nW = 0.98664, p-value = 0.9896\r\n\r\n\r\n\r\ncheck_heteroskedasticity(Tanino) # Breusch-Pagan test (1979).\r\n\r\n\r\nOK: Error variance appears to be homoscedastic (p = 0.843).\r\n\r\n\r\n\r\ncheck_outliers(Tanino)\r\n\r\n\r\nOK: No outliers detected.\r\n- Based on the following method and threshold: cook (0.77).\r\n- For variable: (Whole model)\r\n\r\nExaminamos un modelo sin el dato extremo:\r\n\r\n\r\nSindat7 <- lm(growth[-7] ~ tannin[-7]) \r\nsummary(Sindat7)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth[-7] ~ tannin[-7])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.4549 -0.9572 -0.1622  0.4572  2.6622 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.6892     0.8963  13.042 1.25e-05 ***\r\ntannin[-7]   -1.1171     0.1956  -5.712  0.00125 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.457 on 6 degrees of freedom\r\nMultiple R-squared:  0.8446,    Adjusted R-squared:  0.8188 \r\nF-statistic: 32.62 on 1 and 6 DF,  p-value: 0.001247\r\n\r\nNo ganamos gran cosa\r\nExtra\r\nPodemos obtener los resultados del modelo lineal en formato de\r\ndataframe con el paquete broom(Robinson,\r\nHayes, and Couch 2022) y la función tidy\r\n\r\n\r\ninstall.packages(\"broom\")\r\n\r\n\r\n\r\n\r\n\r\nlibrary(broom)\r\ncoeftable <- tidy(Sindat7)\r\ncoeftable\r\n\r\n\r\n# A tibble: 2 x 5\r\n  term        estimate std.error statistic   p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\r\n1 (Intercept)    11.7      0.896     13.0  0.0000125\r\n2 tannin[-7]     -1.12     0.196     -5.71 0.00125  \r\n\r\nDe esta forma podemos reportarla de manera más ordenada en un\r\ndocumento Rmarkdown, Quarto o simplemente guardarla como un csv para\r\nluego copiarla a mano en un documento word\r\nAsí se puede guardar\r\n\r\n\r\nwrite.csv(coeftable, # objeto a guardar\r\n          file = \"modelo2.csv\") # nombre del archivo\r\n\r\n\r\n\r\nPara predecir valores usamos:\r\n\r\n\r\npredict(Tanino, list(tannin =7.5))\r\n\r\n\r\n       1 \r\n2.630556 \r\n\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nls() \r\n\r\n\r\n[1] \"coeftable\" \"i\"         \"modelito\"  \"Nulo\"      \"reg.data\" \r\n[6] \"Sindat7\"   \"Tanino\"    \"ysomb\"    \r\n\r\nrm(list=ls(all=TRUE))\r\n\r\n\r\n\r\nFin\r\n\r\n\r\n\r\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip\r\nWaggoner, and Dominique Makowski. 2021.\r\n“Performance: An\r\nr Package for Assessment, Comparison and\r\nTesting of Statistical Models” 6: 3139. https://doi.org/10.21105/joss.03139.\r\n\r\n\r\nRobinson, David, Alex Hayes, and Simon Couch. 2022. “Broom:\r\nConvert Statistical Objects into Tidy Tibbles.” https://CRAN.R-project.org/package=broom.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T22:19:22-06:00"
    },
    {
      "path": "Tarea_1.html",
      "title": "Tarea 1",
      "author": [],
      "contents": "\r\n\r\nContents\r\nTeoréma de tendencia\r\ncentral\r\n\r\nTeoréma de tendencia central\r\nSupongamos que tenemos una muestra de 100 colibríes de una especie A.\r\nLes hemos medido la extensión de alas y encontramos que la media es 17\r\ncm y la desviación estándar es de 0.8 cm.\r\nDibujen la distribución de probabilidad bajo un supuesto de\r\nnormalidad\r\n¿que probabilidad hay de encontrar un caso con una extensión\r\nmayor a 18cm?\r\nNoten que cualquier valor de y en una distribución normal se puede\r\nconvertir en un valor de z. Recuerden que \\(z=(y-media(y))/desv.stand\\).\r\n¿y la de encontrar un ave con una extensión menor a 15?\r\n¿ustedes creen que un ave con una extensión de 15cm puede decirse\r\nque con una confianza del 95% pertenece a la misma población?\r\n¿entre que medidas de extensión de alas se puede decir con un 95%\r\nde confianza que las aves pertenecen a esa población?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T11:44:56-06:00"
    },
    {
      "path": "Tarea_2.html",
      "title": "Tarea 2",
      "author": [],
      "contents": "\r\n\r\nContents\r\nEstimación\r\nPreguntas:\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nEstimación\r\nLa tabla de datos de Palmer.xlsx contiene los Valores de\r\nImportancia (VI) de 25 especies de briofitas (musgos y líquenes)\r\ncreciendo en troncos de árboles en tres sitios en el Duke Forest, en\r\nCarolina del Norte. Los VIs son una medida de abundancia relativa. Cada\r\nvalor (por celda) es el promedio de 10 árboles por sitio.\r\nLos códigos de las muestras indican las especies de árbol:\r\nBN = Betula nigra, abedul;\r\nLT = Liriodendron tulipifera, árbol de tulipán;\r\nPE = Pinus echinata, pino de hoja chica;\r\nPO = Platanus occidentalis, ficus;\r\nPT = Pinus taeda, pino hoja mediana;\r\nQR= Quercus rubra, encino rojo;\r\nQA= Quercus alba, encino blanco.\r\nPreguntas:\r\n¿Cuál crees que sea la estimación más eficiente de la tendencia\r\ncentral para Isopterygium tenerum?\r\nEstima con un 93% de confianza en que intervalo se encuentra la\r\nmedia para 4 especies de briofitos (utilicen un método basado en\r\ndistribuciones y uno en remuestreo).\r\n¿Qué puedes decir para las distintas especies? ¿difieren las\r\nestimaciones dependiendo del método? ¿Porqué?\r\n¿Cómo reportarías la media general para Platygyrium repens en un\r\nartículo científico?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-17T12:13:18-06:00"
    },
    {
      "path": "Tarea_3.html",
      "title": "Tarea 3",
      "author": [],
      "contents": "\r\n\r\nContents\r\nAnálisis Exploratorios\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nAnálisis Exploratorios\r\nLa tabla de datos de Palmer.csv contiene los valores de\r\nimportancia de 25 especies de briofitas (musgos y líquenes) creciendo en\r\ntroncos de árboles en tres sitios en el Duke Forest, en Carolina del\r\nNorte. Los valores de importancia son una medida de abundancia relativa.\r\nCada muestra representa el promedio de 10 árboles en un sitio dado. Los\r\ncódigos de las muestras indican la especies de árbol y el sitio; BN =\r\nBetula nigra, abedul; LT = Liriodendron tulipifera, árbol de tulipán; PE\r\n= Pinus echinata, pino de hoja chica; PO = Platanus occidentalis, ficus;\r\nPT = Pinus taeda, pino hoja mediana; QR= Quercus rubra, encino rojo; QA=\r\nQuercus alba, encino blanco.\r\nPlantear una pregunta sencilla y examinar, a partir del\r\nexploratorio establecer si tengo la información suficiente para\r\ncontestar o no esa pregunta.\r\nRealizar un análisis exploratorio completo de los datos. Usando\r\nlos datos de la tabla anterior.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T13:24:58-06:00"
    },
    {
      "path": "Tarea_4.html",
      "title": "Tarea 4",
      "author": [],
      "contents": "\r\n\r\nContents\r\nRemuestreo\r\nen pruebas de hipótesis con una muestra simple\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nRemuestreo\r\nen pruebas de hipótesis con una muestra simple\r\nEstos son datos de Michelson (1978) de medidas tomadas para estimar\r\nla velocidad de la luz que ahora sabemos es (299,792.458 km p seg).\r\nUse el cuerpo de datos contenidos en light.txt y\r\nutilizando el método de remuestreo calcule los intervalos de confianza\r\n(95%) de la media de la velocidad (speed). ¿El valor medio está dentro\r\ndel intervalo de confianza del muestreo?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T13:22:56-06:00"
    },
    {
      "path": "Tarea_5.html",
      "title": "Tarea 5",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPrueba\r\nde hipótesis\r\n\r\nPrueba de hipótesis\r\nUn investigador está interesado en probar si las hormigas construyen\r\npreferentemente sus nidos en árboles de una de dos especies. Muestreó\r\n100 árboles de cada una de las especies y encontró que 60 árboles de la\r\nespecie A y 20 de la especie B tenían nidos. ¿Esta muestra apoya la\r\nhipótesis de preferencia?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T13:21:45-06:00"
    },
    {
      "path": "Tarea_6.html",
      "title": "Tarea 6",
      "author": [],
      "contents": "\r\n\r\nContents\r\nRegresión\r\nsimple\r\nRecuerda\r\n\r\n\r\nNOTA: Para descargar los datos, dar click en el boton. Aparece\r\nuna pantalla con los datos, luego dar click derecho y poner guardar como\r\n(save as). Guardar el txt donde van a trabajar\r\n\r\n\r\n Descargar datos\r\n\r\n\r\nRegresión simple\r\nLos datos de masas esqueléticas y corporales (todas en kg) del\r\narchivo pesoaves.txt fueron tomados al azar de un estudio\r\nmás amplio realizado por Prange, Anderson, and Rahn (1979) donde\r\nanalizaban la hipótesis de que las aves tienen esqueletos más ligeros\r\nque los mamíferos debido a su adaptación al vuelo. Las dos primeras\r\ncolumnas correponden a masas de esqueleto y de cuerpo de aves\r\nrespectivamente. Las dos últimas columnas a masas esquelética y\r\ncorporal, respectivamente, de mamíferos.\r\nEncuentra un modelo mínimo adecuado que represente la relación\r\nalométrica entre masa esquelética y masa corporal para aves por un lado\r\ny para mamíferos por el otro. ¿Cómo reportaría los resultados obtenidos\r\nen un artículo científico?\r\nEstima con dichos modelos cuál sería la masa esquelética esperada\r\npara un ave de 5 kg de masa corporal y cuál sería la masa esquelética de\r\nun mamífero de 5 kg.\r\nEn un análisis cualitativo (por ejemplo sobreponiendo las curvas\r\npredichas) crees que se sostiene la hipótesis de los autores?\r\nRecuerda\r\n• Realizar el análisis exploratorios (gráficos y tablas)\r\npertinentes.\r\nEjecutar los análisis en R, por favor incluye únicamente las\r\nsalidas del programa que son ESTRICTAMENTE NECESARIAS\r\npara respaldar tus decisiones -\r\nEs INÚTIL incluir en el reporte salidas del\r\nprograma SIN INTERPRETACIÓN.\r\n\r\n\r\n\r\nPrange, Henry D., John F. Anderson, and Hermann Rahn. 1979.\r\n“Scaling of Skeletal Mass to Body Mass in Birds and\r\nMammals.” The American Naturalist 113 (1): 103–22. https://doi.org/10.1086/283367.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-01-19T17:31:28-06:00"
    },
    {
      "path": "try.html",
      "title": "Videos Semana 2",
      "author": [],
      "contents": "\r\nhttps://drive.google.com/file/d/1J6ELC-oqQrsTNsTsSGfVGyV8iBELnJGG/view?usp=sharing\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-17T15:49:11-06:00"
    }
  ],
  "collections": []
}
